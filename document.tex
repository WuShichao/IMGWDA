\documentclass[openleft]{kentigern}

\usepackage{lipsum}
\usepackage[acronym,toc]{glossaries}
\usepackage{siunitx}

\makeglossaries
\usepackage{tabularx}
\usepackage{type1cm}
\usepackage{lettrine}
\usepackage{physicsplus}
\usepackage{caption}

\usepackage{rotating}
\usepackage{pgfgantt}

\usepackage{setspace}
    \linespread{1.25}

\title{Year 2 Report}
\author{Daniel Williams}

\input{macros/macros}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\loadglsentries[main]{glossary/glossary.tex}


\usetikzlibrary{bayesnet}

\begin{document}
%\maketitle

\begin{titlepage}
\newgeometry{margin=2in}
\thispagestyle{empty}

\maketitle
\restoregeometry
\end{titlepage}

\newpage
%

%\tableofcontents
\newpage
%\input{tex/dedication}
% \part{Outline \& Review of Gravitational Wave Literature}
% \label{part:introduction}


\part{Introduction}
\label{part:intro}


 \chapter{Gravitational Waves}
 \label{cha:grav-waves}

%\section{Gravitational waves}
%\label{sec:gravwaves}
\input{tex/0-gravitationalwaves/0-1-introduction}


% \section{Gravitational waves and general relativity}
% \label{sec:grav-waves-gener}
% \input{tex/0-gravitationalwaves/0-2-generalrelativity}

\section{GW150914: The first detection}
\label{sec:gw150914:-first-dete}
\input{tex/0-gravitationalwaves/0-3-gw150914}


 \chapter{Detectors}
 \label{cha:detectors}

 \chapterprecis{Gravitational wave detectors are one of the great
   achievements of Twentieth and early Twenty-First Century
   science. They are the most sensitive measuring devices ever
   constructed, but they face numerous technical challenges.}

%\section{Gravitational wave detectors}
%\label{sec:detectors}

\input{tex/1-detectors/1-1-introduction}

% \section{Detector architectures}
% \label{sec:detect-arch}

% \section{Resonant bar detectors}
% \label{sec:reson-bar-detect}

 % \section{Ground-based interferometers}
 % \label{sec:ground-based-interf}
 % \input{tex/1-detectors/ground-based}


% \subsection{Space-based interferometers}
% \label{sec:space-based-interf}

% \subsection{Earth normal modes}
% \label{sec:earth-normal-modes}

% \subsection{Spacecraft telemetry}
% \label{sec:spacecraft-telemetry}



 % \section{Noise sources}
 % \label{sec:an-overview-noise}
 % \input{tex/1-detectors/noise}


 \chapter{Sources of Gravitational Waves}
 \label{cha:sourc-grav-waves}

 \chapterprecis{Gravitational waves are produced by any situation
   containing accelerating masses which are arranged in an asymmetrical
   manner, for example binary star systems, or non-spherical pulsars. A
   wide range of astrophysical sources are capable of producing
   gravitational waves, although only a handful of these are likely to
   be luminous enough to detect, or produce radiation over a frequency
   band which can be detected by current-generation detectors.}

%\section{Sources of gravitational radiation}
%\label{sec:sources}


\input{tex/2-sources/introduction}

 \section{Compact Binary Coalescence}
 \label{sec:cbc}
 \input{tex/2-sources/2-1-cbc}

 \section{Supernovae}
 \label{sec:sn}
 \input{tex/2-sources/2-2-cc-supernovae}

 \section{Gravitational-wave Pulsars}
 \label{sec:pulsar}
 \input{tex/2-sources/2-3-pulsar}

 \section{Stochastic Sources}
 \label{sec:stochastic-sources}
 \input{tex/2-sources/2-4-stochasticbackground}

% \chapter{Bayesian inference}
% \label{cha:bayesian-inference}
% \input{tex/3-analysis/bayesian-inference}

% \chapter[Data Analysis]{Analysis of \\gravitational wave data}
% \label{cha:data-analys-grav}

%  \chapterprecis{Identifying gravitational wave signals in
%    interferometer data is not a trivial task, with signals appearing at
%    low signal-to-noise ratios.}

% \section{Matched filtering}
% \label{sec:matched-filtering}
% \input{tex/3-analysis/matched-filtering}

% \section{Pipeline overview}
% \label{sec:pipeline-overview}
% \input{tex/3-analysis/pipeline-overview}

% \section{Numerical Relativity}
% \label{sec:numerical-relativity}
% \input{tex/3-analysis/numerical-relativity}

\part{Data Analysis for Gravitational Wave Detectors}
\label{part:data-analysis}

\chapter{Gaussian processes}
\label{cha:gaussian-process}

\chapterprecis{ Gaussian processes are a powerful machine learning
  tool capable of performing multi-dimensional regression in a
  generalised Bayesian framework, which can be used as priors for further analysis operations.}

\input{tex/3-analysis/gaussian-processes}

%\appendices

% \chapter{Covariance Functions}
% \label{cha:covariance}

% \chapterprecis{An entire menagerie of covariance functions for }

% \part{Summary of work thus far}
% \label{part:work}

% \chapter{Presentations \& publications}
% \label{chap:pandp} \newpage


\chapter{Work thus far}
\chapterprecis{}
The primary focus of my work thus far has been on the development of a
Gaussian process-based method for determining the optimal parameters
of future numerical relativity simulations. This has the added
spin-off benefit of providing a means of producing waveform
approximants which are trained directly off numerical relativity data.

I have also been involved in work to determine a Bayesian lower limit
on the beaming angle of soft GRBs from the observed rate posterior
probability distribution of neutron star merger events in advanced
LIGO. I develop and maintain a software package, ``Minke'', which is
used within the LIGO Scientific Collaboration (LSC) to generate the
necessary data for testing and evaluating transient event searches
(through software ``injections'', or mock data challenges), and for
producing ``hardware injections'' which are used to test the effect of
various detector calibrations on these searches.

Additionally, in the last year I have presented at one conference, and
been a member of the local organising committee for an LSC
collaboration meeting which was held at the University of Glasgow in
September 2016. I have written two popular science articles which have
been published, one of which was awarded the College of Science and
Engineering Science Writing Prize, and been involved with the
Education and Public Outreach group within the LIGO Scientific
Collaboration, contributing materials for events surrounding the
announcement of GW151226 in June 2016, and assisting with the running
of the collaboration's Twitter account. Since March 2016 I have been
on long term attachment to the LIGO site in Livingston, Louisiana.

\section{Gaussian processes for gravitational wave detection}
\label{sec:gauss-proc-grav}

\begin{figure}
  \centering
  \includegraphics{figures/georgiatech-density-plot.pdf}
  \caption{A pair-plot of the parameter space sampling in the Georgia
    Tech catalogue. The subplots on the diagonals show the histograms
    of the distribution of waveforms generated with respect to each
    individual parameter; the Gaussian kernel density estimate is also
    plotted on these panels.}
  \label{fig:georgiatech-pairplot}
\end{figure}
Gaussian processes have the attractive feature that they can be
leveraged to produce a regression of data, which has both the
equivalent of a ``best-fit'' line, which is represented by the mean
function from the posterior, and an uncertainty, through the variance
of the posterior. In matched filtering techniques used in LIGO data
analysis it is often impractical to use numerical relativity (NR)
waveforms, owing to their computational expense, and so approximants
must be used to span the regions of parameter space between the
``true'' NR waveforms.

Moore and Gair\cite{2014PhRvL.113y1101M,2016PhRvD..93f4001M} propose a
method to account for the uncertainty which is introduced through the
use of an approximant, by training a Gaussian process on the
differences between NR and approximant waveforms. They then use the
posterior from the Gaussian process to form the likelihood in their
analysis. While this approach is shown to improve the efficiency of
bayesian inference techniques on parameter estimation, it relies on
the availability of an analytic or a semi-analytic approximant to the
true values.


Instead, I have attempted to take the approach of training a Gaussian
process directly using the numerical relativity waveforms produced by
the Relativity Group at Georgia Tech\cite{gatechcat}: here the input
parameters are multi-dimensional, composed of the time, and the
simulation parameters, and the strain values at each of these points
composing the targets. The distribution of the waveforms in the
catalogue is plotted in figure \ref{fig:georgiatech-pairplot}, which
shows both how uneven the sampling of the underlying parameter space
is, and the low density of the sampling which necessitates the use of
approximants to fully cover the parameter space.

This approach allows two useful results to be obtained: first, it
allows the production of gravitational waveforms, with associated
error estimates, directly from the NR data, bypassing the need for an
approximant; second, a fully trained Gaussian process, through its
ability to estimate the uncertainty in any prediction, provides a
means to indetify regions of parameter space where predictions are
likely to be particuarly poor, and is thus a means of identifying the
optimal regions of parameter space to focus future simulation efforts
on.

\begin{figure}
  \centering
  \includegraphics[scale=0.35]{figures/spacings.pdf}
    \caption{A corner plot of a `hyperslice' the parameter space of the numerical relativity waveforms centred on $(t=0,  q=1.5,    a_1=0.8,    a_2=0.8,   \theta_{1L}=60. ,  \phi_1 = 180. ,   \theta_{12} = 30. ,   \theta_{SL} = 75. ,   \theta_{JL} = 22. )$, showing the variation in the magnitude of the Gaussian process uncertainty over the parameter space in the colorplot, and the optimal spacing as implied by the width of the covariance function. The plots above each column are designed to provide a guide to the density of samples throughout the parameter space in that dimension, while the red point represents the point of intersection of the various planes. The Gaussian process used to produce these uncertainty estimates was trained off entire waveforms, including time domain information, but represents a series of slices which intersect at $(t=0,  q=1.5,    a_1=0.8,    a_2=0.8,   \theta_{1L}=60. ,  \phi_1 = 180. ,   \theta_{12} = 30. ,   \theta_{SL} = 75. ,   \theta_{JL} = 22. )$}
  \label{fig:optimal-spacings}
\end{figure}

The magnitude of the Gaussian process error provides a means of
detecting regions of the parameter-space which are poorly understood,
and regions of high uncertainty should then be targeted for future
simulations in order to improve the validity of the surrogate function
across the entire parameter space. The values of $\lambda_{ab}$ define
a metric on the parameter space, and thus a suggested spacing for
these new waveforms, which should be sampled at intervals of
$\log(\lambda_{ab})$, for $\lambda_{ab} \in \Lambda$, in regions with
high uncertainty or regions outwith the parameter-space region defined
by the original training data. Figure \ref{fig:optimal-spacings} shows an
9-dimensional slice in the parameter space of the BBH waveforms from a
Gaussian process trained off ten waveforms. We can see that even with
a small number of waveforms it is possible to provide some estimate of
the correct grid sampling scale-length required to complete the
model\footnote{For simplicity a hypercube lattice is illustrated in
  figure \ref{fig:optimal-spacings}, however, more efficient lattice patterns
  exist to which the scale-length can be applied.}.

In order to demonstrate that the parameter spacing is reasonably
independent of the number of waveforms used in training the GP the
model was generated with a differing number of training waveforms, and
trained. The scale length for each parameter is found to be
consistently similar, as can be seen in figure \ref{fig:convergence}.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/convergence.pdf}
  \caption{The difference in the scale-lengths of the trained Gaussian process given 10, 29, 72, 164, and 324 waveforms, compared to the GP trained from 10 waveforms.}
  \label{fig:convergence}
\end{figure}

Having established the optimal spacing of future waveforms we must
then turn to the question of the optimal order in which they should be
created. Sampling the entire parameter space at the suggested density
would require in excess of 250,000 waveforms, which is an unachievably
large quantity of data.

In other fields which exploit Gaussian processes the choice of future
sample locations can be made using techniques from Bayesian
optimisation. In these fields the Gaussian process is often emulating
the behaviour of a complicated function surface, and the desired
outcome is finding an optimum on this surface, for example, if a GP is
used to model the distribution of pollutants in a lake, with the aim
of identifying the source of the pollution we may want to find the
region with the maximum amount of pollutant. 

In the case of gravitational waveform modelling we are not terribly
interested in the location of the function's maximum, but instead have
the objective of achieving the greatest quantity of knowledge possible
in the shortest possible period of time (or equivalently, with the
smallest number of function evaluations). In this case the choice of
future samples might be made to minimise the mean-squared error of the
model.

Using methods based-on the use of testing data requires some of the
available training data to be set-aside, and not used to train the
Gaussian process. Given the small number of samples available, and the
large volume of the parameter space this is likely to have a
considerable impact on the predictive capability of the model. Other
options are available, such as comparing the output to an analytical
model, for example \texttt{IMRPhenomP}, but these approaches suffer
from the incomplete understanding of the uncertainties in these
models.

One possible approach is to simply use the metric defined on the
parameter space by the Gaussian process to determine the location
which is geometrically furthest from any pre-existing training point
in the parameter space. A search of this type can be done efficiently
using voronoi tesselations to rapidly indentify points in the
parameter space which are most distant from any other point. This
approach is clearly \emph{exploration}-driven, where the parameter
space is sampled as widely as possible to get an overview of the
entire underlying function.

An alternative approach is to again use the grid spacing to identify
the closest location to the pre-existing samples which has an
uncertainty greater than some pre-defined threshold in the Gaussian
process model. This approach would fill-in ``holes'' in the parameter
space, working outwards from the existing samples.

A third approach is to allow this planning stage to be informed by the
population distribution of observed signals, and to attempt to improve
the model in order to produce the highest quality waveforms in the
regions of parameter space which are known to represent actualy BBH
systems, that is, which have parameters comparable to those seen in
previous observations.



In the last year I've developed additionally developed a working
protoype of multiple-input, multiple-output Gaussian process
regression in Python, although due to the considerable computational
resources required I have yet to apply this to real-world
data. However, this approach should allow the production of a Gaussian
process surrogate model which is capable of producing both
polarisations of the gravitational waveform for a BBH event. A number
of computational improvements could be made, including parallelising
the training process, which may make this possible.

I am currently in the process of writing-up the initial stages of this
work (the method of using a trained Gaussian process surrogate for
waveform placement) into a paper.

\section{Minke: Producing MDCs for gravitational wave transient analysis}
\label{sec:mink-prod-mdcs}

In order to characterise the efficiency of any gravitational wave
analysis algorithm it must be tested on injected signals---simulated
signals with known parameters, and hence a known shape---which are
inserted into either simulated noise, or timeseries containing real
noise from the detector. By determining the number of these injected
signals which are detected by an algorithm it is possible to determine
the detection efficiency of the method; additionally, this method
allows a controlled method of determining the quietest events the
algorithm is capable of detecting in the noise environment, and hence
of calculating the sensitive distance of the detector and analysis to
a physical system producing a waveform with those parameters. These
tests are known as ``Mock Data Challenges'' (MDCs).

So that consistent results can be produced between the various
different search pipelines it is necessary to generate these
injections via an independent method; previous methods for doing this
within the LSC's burst group have been difficult to maintain or
ad-hoc; ``Minke'' is an attempt to produce an extensible and pythonic
framework for producing burst signal sets, either for use in MDCs, or
for other purposes, such as training machine learning classifiers.

The majority of the initial development on Minke was carried out in
the first six months of my PhD, however I have continued its
development over the last year, with major early achievements being
the implementation and testing of supernova numerical relativity
waveform support, where supernova waveforms which are pre-calculated
by numerical relativity simulations can be injected into detector
data, and the review of the code prior to the pulication of the O1
Burst Search paper\cite{2017PhRvD..95d2003A}, which made use of MDC
sets produced by Minke. Verion 1.0 of ``Minke'' was released on 14
September 2016\footnote{``Minke'' is open source, and both the
  source-code and releases are available on Github at
  \www{http://www.github.com/transientunatic/minke}.}.
During the same period I have been a co-author on a short-author list
paper, and the lead author on a paper composing the write-up of my
MSci project.

I had initially planned development to continue in the direction of
adding support for numerical relativity BBH waveforms to the package,
however this proved difficult due to the lack of a firm standard for
the storage of these waveforms (this standard was finally agreed upon
in March 2017 \cite{2017arXiv170301076S}, and so adding support for
these waveforms has become a short-term goal in the future. Instead I
have focussed further development on adding support for numerical
models such as accretion disk instabilities to the package, and the
ability to generate data which can be used to generate hardware
injections in the detectors. At the time of writing neither of these
new features had been fully reviewed, however review of at least the
hardware injection feature is likely to be required prior to the
publication of a future joint O1/O2 supernova search paper.


\section{Estimating beaming angles from sGRBs}
\label{sec:estim-beam-angl}

\sidebar{

  \begin{tikzpicture}
      % Y
  \node[latent]  (theta)   {$\theta$}; %

  % input distributions
  \node[obs, above = 1.2 of theta] (Rbns)   {$R_{\mathrm{bns}}$}; %
  
  \node[latent, left = of Rbns]  (eff)   {$\epsilon$}; %
  \node[obs, right = of Rbns] (Rgrb) {$R_{\mathrm{grb}}$};
  % eff hyperparameters
  \node[const, above=1.2 of eff, xshift=-0.5cm] (mw) {$h_1$} ; %
  \node[const, above=1.2 of eff, xshift=0.5cm]  (aw) {$h_2$} ; %
  % Rbns hyperparameters
  %\node[const, above=1.2 of Rbns, xshift=-0.5cm] (mx) {$\mu_x$} ; %
  %\node[const, above=1.2 of Rbns, xshift=0.5cm]  (ax) {$\alpha_x$} ; %
   % Factors
  \factor[above=of eff] {eff-f} {left:$\mathcal{D}$} {mw,aw} {eff} ; %
  \edge[-] {eff, Rbns} {theta};
  \edge[-] {Rgrb} {theta};
\end{tikzpicture}
\captionof{figure}{A graphical representation of the hierarchical model used to infer the beaming angle of soft gamma ray bursts from the observed binary neutron star coalesence rate. \label{fig:grb-graph-model}}
}
I have been working in collaboration with James Clark at Georgia
Institute of Technology on developing a method for estimating the
lower-limit on the beaming angle of soft gamma ray bursts (sGRBs)
which result from binary neutron star (BNS) mergers.

This is made possible thanks to knowledge of the posterior probability
distribution on the rate of BNS mergers, which can be determined from
the number of such events which are observed by Advanced LIGO
(although, at the time of writing, this number was zero), and
knowledge of the rate of observed soft gamma ray bursts. A
hierarchical Bayesian analysis can then be used to infer the
probability distribution on the ``opening angle'' of the sGRB.

We can model the observed distribution of gamma ray bursts given
knowledge of the rate of binary neutron star (BNS) mergers as
\begin{equation}
  \label{eq:grb-rate}
  \mathcal{R}_{\mathrm{grb}} = \epsilon \mathcal{R}_{\mathrm{bns}} \left\langle 1 - \cos\theta \right\rangle,
\end{equation}
with $\mathcal{R}_{\mathrm{grb}}$ the observed rate of gamma ray
bursts, $\mathcal{R}_{\mathrm{bns}}$ the observed rate of binary
neutron star coalescences, $\epsilon$ the efficiency at which a BNS
produces a GRB, and $\theta$ the beaming angle. Since two of these
quantities are observable, we can infer $\theta$ by placing a prior on
$\epsilon$, producing a model as depicted in figure
\ref{fig:grb-graph-model}, where $\mathcal{D}(h_1, h_2)$ represents
some prior distribution with two hyperparameters.

\begin{figure}
  \centering
  \includegraphics{figures/grbbeamingpymc.pdf}
  \caption{Distributions in the modelling of the GRB beaming angle
    given two observing scenarios; the first aLIGO observing run (O1)
    in which no BNS events were seen, and a speculative, longer second
    observing run (O2) in which one event is seen. Each column
    represents evaluation of the model with a different prior, which
    is specified at the top of the column. Between the top two rows an
    additional conversion is made between the observed signal rate and
    the actual BNS rate.}
  \label{fig:grb-distributions}
\end{figure}

The distribution on $\theta$ can then be inferred using an MCMC
process to sample the posterior. Results for an evaluation of such a
model are shown in figure \ref{fig:grb-distributions}.


%
% A little more detail here would not go amiss
% 

\section{Filter Modeling for glitch tracing in Advanced LIGO}
\label{sec:glitchtracing}

During the course of my long-term attachment at the LIGO site at
Livingston, Louisiana I have been working on developing a system for
modelling the digital signal processing system employed during data
acquisition and detector control at the site. When one of the filters
in this system becomes unstable it can produce a ``glitch'', a
transient noise event in the data from a sensor, or, in the worst
cases, in the DARM read-out channel, which is used to calculate the
gravitational wave strain observed by the detector.

The majority of the digital signal processing system is designed using
Simulink, and the entire system consists a few hundred models which
work in parallel, and which combine multiple input signals. As a
result, tracing the source of a signal which produces a glitch is
non-trivial, and the structure of the signal processing chain acts to
obfuscate the origin of glitch-producing faults.

The signal processing system can be modelled as a directed graph,
allowing graph theoretic algorithms to be used to traverse the system,
and trace the processing of a signal across numerous Simulink models
in order to identify the cause of filter instabilities. My work at
LIGO thus far has mostly revolved around development of a Python
codebase which is capable of parsing Simulink models and filter
definition files in order to run such an analysis.


\appendices

\chapter{Outline of future work}
\label{part:future}
%\chapterprecis{}
\input{tex/5-future}
\backmatter

\newgeometry{left=3cm}

\bibliographystyle{unsrt}
\bibliography{bibliography/introduction,bibliography/relativity,bibliography/detectors,bibliography/gw150914,bibliography/sources,bibliography/analysis,bibliography/gaussian}

% The glossary
\input{glossary/glossary.tex}
%\input{glossary/sources-glossary.tex}

\glsaddall
\useglossarystyle{altlist}
\printglossaries

\end{document}
