@BOOK{barberBRML2012,
author = {Barber, D.},
title= {{Bayesian Reasoning and Machine Learning}},
publisher = {{Cambridge University Press}},
year = 2012}
@Book{rasmussen2006gaussian,
 author = {Rasmussen, Carl},
 title = {Gaussian processes for machine learning},
 publisher = {MIT Press},
 year = {2006},
 address = {Cambridge, Mass},
 isbn = {026218253X}
 }
@ARTICLE{2016arXiv160401250M,
   author = {{Moore}, C.~J. and {Chua}, A.~J.~K. and {Berry}, C.~P.~L. and 
	{Gair}, J.~R.},
    title = "{Fast methods for training Gaussian processes on large data sets}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1604.01250},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Statistics - Computation, Statistics - Methodology},
     year = 2016,
    month = apr,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160401250M},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016PhRvD..93f4001M,
   author = {{Moore}, C.~J. and {Berry}, C.~P.~L. and {Chua}, A.~J.~K. and 
	{Gair}, J.~R.},
    title = "{Improving gravitational-wave parameter estimation using Gaussian process regression}",
  journal = {\prd},
archivePrefix = "arXiv",
   eprint = {1509.04066},
 primaryClass = "gr-qc",
     year = 2016,
    month = mar,
   volume = 93,
   number = 6,
      eid = {064001},
    pages = {064001},
      doi = {10.1103/PhysRevD.93.064001},
   adsurl = {http://adsabs.harvard.edu/abs/2016PhRvD..93f4001M},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014PhRvL.113y1101M,
   author = {{Moore}, C.~J. and {Gair}, J.~R.},
    title = "{Novel Method for Incorporating Model Uncertainties into Gravitational Wave Parameter Estimates}",
  journal = {Physical Review Letters},
archivePrefix = "arXiv",
   eprint = {1412.3657},
 primaryClass = "gr-qc",
 keywords = {Gravitational wave detectors and experiments},
     year = 2014,
    month = dec,
   volume = 113,
   number = 25,
      eid = {251101},
    pages = {251101},
      doi = {10.1103/PhysRevLett.113.251101},
   adsurl = {http://adsabs.harvard.edu/abs/2014PhRvL.113y1101M},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@InProceedings{EPFL-CONF-161319,
   abstract    = {We present a framework for sparse Gaussian process (GP)
                 methods which uses forward selection with criteria based
                 on information-theoretic principles, previously suggested
                 for active learning. Our goal is not only to learn
                 d-sparse predictors (which can be evaluated in O(d)
                 rather than O(n), d much smaller than n, n the number of
                 training points), but also to perform training under
                 strong restrictions on time and memory requirements. The
                 scaling of our method is at most O(n d^2), and in large
                 real-world classification experiments we show that it can
                 match prediction performance of the popular support
                 vector machine (SVM), yet can be significantly faster in
                 training. In contrast to the SVM, our approximation
                 produces estimates of predictive probabilities ("error
                 bars"), allows for Bayesian model selection and is less
                 complex in implementation.},
   affiliation = {OTHER},
   author      = {Lawrence, Neil and Seeger, Matthias and Herbrich, Ralf},
   booktitle   = {Proceedings of the 16th {A}nnual {C}onference on
                 {N}eural {I}nformation {P}rocessing {S}ystems},
   details     = {http://infoscience.epfl.ch/record/161319},
   documenturl = {http://infoscience.epfl.ch/record/161319/files/ivm.pdf},
   keywords    = {Gaussian process; Sparse approximation; Informative vector machine},
   location    = {Vancouver, BC},
   oai-id      = {oai:infoscience.epfl.ch:161319},
   oai-set     = {conf},
   pages       = {609--616},
   review      = {REVIEWED},
   status      = {PUBLISHED},
   submitter   = {208475},
   title       = {Fast {S}parse {G}aussian {P}rocess {M}ethods: {T}he
                 {I}nformative {V}ector {M}achine},
   unit        = {LAPMAL},
   year        = 2003
}

@article{hodlr,
    author = {{Ambikasaran}, S. and {Foreman-Mackey}, D. and
              {Greengard}, L. and {Hogg}, D.~W. and {O'Neil}, M.},
     title = "{Fast Direct Methods for Gaussian Processes and the Analysis
               of NASA Kepler Mission Data}",
      year = 2014,
     month = mar,
       url = http://arxiv.org/abs/1403.6015
}
@book{mackay2003information,
  title={Information theory, inference and learning algorithms},
  author={MacKay, David JC},
  year={2003},
  publisher={Cambridge university press}
}
@article{Alvarez2009,
author = {{\'{A}}lvarez, MA and Lawrence, ND},
file = {:home/daniel/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/{\'{A}}lvarez, Lawrence - 2009 - Sparse convolved multiple output gaussian processes.pdf:pdf},
journal = {arXiv preprint arXiv:0911.5107},
title = {{Sparse convolved multiple output gaussian processes}},
url = {http://arxiv.org/abs/0911.5107},
year = {2009}
}
@article{Alvarez2010,
author = {Alvarez, MA and Luengo, D},
file = {:home/daniel/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Alvarez, Luengo - 2010 - Efficient multioutput Gaussian processes through variational inducing kernels.pdf:pdf},
journal = {International  {\ldots}},
title = {{Efficient multioutput Gaussian processes through variational inducing kernels}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/AISTATS2010{\_}AlvarezLTL10.pdf},
year = {2010}
}
@article{Alvarez2011,
author = {{\'{A}}lvarez, Mauricio A. and Lawrence, Neil D.},
issn = {1532-4435},
journal = {The Journal of Machine Learning Research},
month = {feb},
pages = {1459--1500},
publisher = {JMLR.org},
title = {{Computationally Efficient Convolved Multiple Output Gaussian Processes}},
url = {http://dl.acm.org/citation.cfm?id=1953048.2021048},
volume = {12},
year = {2011}
}
@article{Alvarez2011a,
abstract = {Kernel methods are among the most popular techniques in machine learning. From a frequentist/discriminative perspective they play a central role in regularization theory as they provide a natural choice for the hypotheses space and the regularization functional through the notion of reproducing kernel Hilbert spaces. From a Bayesian/generative perspective they are the key in the context of Gaussian processes, where the kernel function is also known as the covariance function. Traditionally, kernel methods have been used in supervised learning problem with scalar outputs and indeed there has been a considerable amount of work devoted to designing and learning kernels. More recently there has been an increasing interest in methods that deal with multiple outputs, motivated partly by frameworks like multitask learning. In this paper, we review different methods to design or learn valid kernel functions for multiple outputs, paying particular attention to the connection between probabilistic and functional methods.},
archivePrefix = {arXiv},
arxivId = {1106.6251},
author = {Alvarez, Mauricio A. and Rosasco, Lorenzo and Lawrence, Neil D.},
eprint = {1106.6251},
file = {:home/daniel/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Alvarez, Rosasco, Lawrence - 2011 - Kernels for Vector-Valued Functions a Review.pdf:pdf},
keywords = {gaussian process},
mendeley-tags = {gaussian process},
month = {jun},
title = {{Kernels for Vector-Valued Functions: a Review}},
url = {http://arxiv.org/abs/1106.6251},
year = {2011}
}
@article{Ambikasaran2014,
abstract = {A number of problems in probability and statistics can be addressed using the multivariate normal (Gaussian) distribution. In the one-dimensional case, computing the probability for a given mean and variance simply requires the evaluation of the corresponding Gaussian density. In the {\$}n{\$}-dimensional setting, however, it requires the inversion of an {\$}n \backslashtimes n{\$} covariance matrix, {\$}C{\$}, as well as the evaluation of its determinant, {\$}\backslashdet(C){\$}. In many cases, such as regression using Gaussian processes, the covariance matrix is of the form {\$}C = \backslashsigma{\^{}}2 I + K{\$}, where {\$}K{\$} is computed using a specified covariance kernel which depends on the data and additional parameters (hyperparameters). The matrix {\$}C{\$} is typically dense, causing standard direct methods for inversion and determinant evaluation to require {\$}\backslashmathcal O(n{\^{}}3){\$} work. This cost is prohibitive for large-scale modeling. Here, we show that for the most commonly used covariance functions, the matrix {\$}C{\$} can be hierarchically factored into a product of block low-rank updates of the identity matrix, yielding an {\$}\backslashmathcal O (n\backslashlog{\^{}}2 n) {\$} algorithm for inversion. More importantly, we show that this factorization enables the evaluation of the determinant {\$}\backslashdet(C){\$}, permitting the direct calculation of probabilities in high dimensions under fairly broad assumptions on the kernel defining {\$}K{\$}. Our fast algorithm brings many problems in marginalization and the adaptation of hyperparameters within practical reach using a single CPU core. The combination of nearly optimal scaling in terms of problem size with high-performance computing resources will permit the modeling of previously intractable problems. We illustrate the performance of the scheme on standard covariance kernels.},
archivePrefix = {arXiv},
arxivId = {1403.6015},
author = {Ambikasaran, Sivaram and Foreman-Mackey, Daniel and Greengard, Leslie and Hogg, David W. and O'Neil, Michael},
eprint = {1403.6015},
file = {:home/daniel/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ambikasaran et al. - 2014 - Fast Direct Methods for Gaussian Processes.pdf:pdf},
month = {mar},
title = {{Fast Direct Methods for Gaussian Processes}},
url = {http://arxiv.org/abs/1403.6015},
year = {2014}
}
@book{Barber2012,
author = {Barber, David},
file = {:home/daniel/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Barber - 2012 - Bayesian Reasoning and Machine Learning.pdf:pdf},
title = {{Bayesian Reasoning and Machine Learning}},
year = {2012}
}
@article{Bonilla2007,
author = {Bonilla, EV and Chai, KM and Williams, C},
file = {:home/daniel/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bonilla, Chai, Williams - 2007 - Multi-task Gaussian process prediction.pdf:pdf},
journal = {Advances in neural  {\ldots}},
title = {{Multi-task Gaussian process prediction}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2007{\_}431.pdf},
year = {2007}
}
@book{MacKay2005,
abstract = {This book is aimed at senior undergraduates and graduate students in Engineering, Science, Mathematics, and Computing. It expects familiarity with calculus, probability theory, and linear algebra as taught in a rst- or secondyear undergraduate course on mathematics for scientists and engineers. Conventional courses on information theory cover not only the beautiful theoretical ideas of Shannon, but also practical solutions to communication problems. This book goes further, bringing in Bayesian data modelling, Monte Carlo methods, variational methods, clustering algorithms, and neural networks. Why unify information theory and machine learning? Because they are two sides of the same coin. In the 1960s, a single eld, cybernetics, was populated by information theorists, computer scientists, and neuroscientists, all studying common problems. Information theory and machine learning still belong together. Brains are the ultimate compression and communication systems. And the state-of-the-art algorithms for both data compression and error-correcting codes use the same tools as machine learning.},
author = {MacKay, David J C},
booktitle = {Learning},
doi = {10.1198/jasa.2005.s54},
file = {:home/daniel/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/MacKay - 2005 - Information Theory, Inference, and Learning Algorithms David J.C. MacKay.pdf:pdf},
isbn = {9780521642989},
issn = {01621459},
title = {{Information Theory, Inference, and Learning Algorithms David J.C. MacKay}},
year = {2005}
}
@book{Rasmussen,
author = {Rasmussen, Carl Edward and Williams, Christopher K I},
file = {:home/daniel/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rasmussen, Williams - Unknown - Gaussian Processes for Machine Learning.pdf:pdf},
isbn = {026218253X},
keywords = {book},
mendeley-tags = {book},
title = {{Gaussian Processes for Machine Learning}}
}
@article{Wilson2011,
abstract = {We introduce a new regression framework, Gaussian process regression networks (GPRN), which combines the structural properties of Bayesian neural networks with the non-parametric flexibility of Gaussian processes. This model accommodates input dependent signal and noise correlations between multiple response variables, input dependent length-scales and amplitudes, and heavy-tailed predictive distributions. We derive both efficient Markov chain Monte Carlo and variational Bayes inference procedures for this model. We apply GPRN as a multiple output regression and multivariate volatility model, demonstrating substantially improved performance over eight popular multiple output (multi-task) Gaussian process models and three multivariate volatility models on benchmark datasets, including a 1000 dimensional gene expression dataset.},
archivePrefix = {arXiv},
arxivId = {1110.4411},
author = {Wilson, Andrew Gordon and Knowles, David A. and Ghahramani, Zoubin},
eprint = {1110.4411},
file = {:home/daniel/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wilson, Knowles, Ghahramani - 2011 - Gaussian Process Regression Networks.pdf:pdf},
month = {oct},
pages = {17},
title = {{Gaussian Process Regression Networks}},
url = {http://arxiv.org/abs/1110.4411},
year = {2011}
}
@misc{,
keywords = {gaussian process,kernel methods,machine learning,massive dataset},
title = {{Approximation methods for gaussian process regression}},
url = {http://research.microsoft.com/pubs/70486/tr-2007-124.pdf},
urldate = {2016-04-28}
}
@misc{,
title = {{Multiple Output Gaussian Pro cess Regression}},
url = {http://www.mcs.vuw.ac.nz/comp/Publications/archive/CS-TR-05/CS-TR-05-2.pdf},
urldate = {2016-05-03}
}
@misc{,
title = {{Gaussian Processes for Timeseries Modelling}},
url = {http://www.robots.ox.ac.uk/{~}sjrob/Pubs/philTransA{\_}2012.pdf},
urldate = {2016-05-03}
}
@book{skilling2006data,
  title={Data analysis: a Bayesian tutorial},
  author={Sivia, D S and Skilling, John},
  year={2006},
  publisher={Oxford University Press}
}

@ARTICLE{2007PhRvD..76d3003C,
   author = {{Clark}, J. and {Heng}, I.~S. and {Pitkin}, M. and {Woan}, G.
	},
    title = "{Evidence-based search method for gravitational waves from neutron star ring-downs}",
  journal = {\prd},
   eprint = {gr-qc/0703138},
 keywords = {Gravitational radiation detectors, mass spectrometers, and other instrumentation and techniques, Gravitational wave detectors and experiments, Data analysis: algorithms and implementation, data management, Neutron stars},
     year = 2007,
    month = aug,
   volume = 76,
   number = 4,
      eid = {043003},
    pages = {043003},
      doi = {10.1103/PhysRevD.76.043003},
   adsurl = {http://adsabs.harvard.edu/abs/2007PhRvD..76d3003C},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{PhysRevLett.106.241101,
  title = {Inspiral-Merger-Ringdown Waveforms for Black-Hole Binaries with Nonprecessing Spins},
  author = {Ajith, P. and Hannam, M. and Husa, S. and Chen, Y. and Br\"ugmann, B. and Dorband, N. and M\"uller, D. and Ohme, F. and Pollney, D. and Reisswig, C. and Santamar\'{\i}a, L. and Seiler, J.},
  journal = {Phys. Rev. Lett.},
  volume = {106},
  issue = {24},
  pages = {241101},
  numpages = {4},
  year = {2011},
  month = {Jun},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.106.241101},
  url = {http://link.aps.org/doi/10.1103/PhysRevLett.106.241101}
}

@Article{gatechcat,
  author = 	 {Jani, K and Healy, J and Clark J and London, L and Laguna, P and Shoemaker, D},
  title = 	 {Georgia Tech Catalog of Gravitational Waveforms},
  journal = 	 {In preparation},
  year = 	 {2016},
  OPTkey = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTpages = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@ARTICLE{2016PhRvD..93d4006H,
   author = {{Husa}, S. and {Khan}, S. and {Hannam}, M. and {P{\"u}rrer}, M. and 
	{Ohme}, F. and {Forteza}, X.~J. and {Boh{\'e}}, A.},
    title = "{Frequency-domain gravitational waves from nonprecessing black-hole binaries. I. New numerical waveforms and anatomy of the signal}",
  journal = {\prd},
archivePrefix = "arXiv",
   eprint = {1508.07250},
 primaryClass = "gr-qc",
     year = 2016,
    month = feb,
   volume = 93,
   number = 4,
      eid = {044006},
    pages = {044006},
      doi = {10.1103/PhysRevD.93.044006},
   adsurl = {http://adsabs.harvard.edu/abs/2016PhRvD..93d4006H},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016PhRvD..93d4007K,
   author = {{Khan}, S. and {Husa}, S. and {Hannam}, M. and {Ohme}, F. and 
	{P{\"u}rrer}, M. and {Forteza}, X.~J. and {Boh{\'e}}, A.},
    title = "{Frequency-domain gravitational waves from nonprecessing black-hole binaries. II. A phenomenological model for the advanced detector era}",
  journal = {\prd},
archivePrefix = "arXiv",
   eprint = {1508.07253},
 primaryClass = "gr-qc",
     year = 2016,
    month = feb,
   volume = 93,
   number = 4,
      eid = {044007},
    pages = {044007},
      doi = {10.1103/PhysRevD.93.044007},
   adsurl = {http://adsabs.harvard.edu/abs/2016PhRvD..93d4007K},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2007CQGra..24S.689A,
   author = {{Ajith}, P. and {Babak}, S. and {Chen}, Y. and {Hewitson}, M. and 
	{Krishnan}, B. and {Whelan}, J.~T. and {Br{\"u}gmann}, B. and 
	{Diener}, P. and {Gonzalez}, J. and {Hannam}, M. and {Husa}, S. and 
	{Koppitz}, M. and {Pollney}, D. and {Rezzolla}, L. and {Santamar{\'{\i}}a}, L. and 
	{Sintes}, A.~M. and {Sperhake}, U. and {Thornburg}, J.},
    title = "{A phenomenological template family for black-hole coalescence waveforms}",
  journal = {Classical and Quantum Gravity},
archivePrefix = "arXiv",
   eprint = {0704.3764},
 primaryClass = "gr-qc",
     year = 2007,
    month = oct,
   volume = 24,
    pages = {S689-S699},
      doi = {10.1088/0264-9381/24/19/S31},
   adsurl = {http://adsabs.harvard.edu/abs/2007CQGra..24S.689A},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2007PhRvD..76j4049B,
   author = {{Buonanno}, A. and {Pan}, Y. and {Baker}, J.~G. and {Centrella}, J. and 
	{Kelly}, B.~J. and {McWilliams}, S.~T. and {van Meter}, J.~R.
	},
    title = "{Approaching faithful templates for nonspinning binary black holes using the effective-one-body approach}",
  journal = {\prd},
archivePrefix = "arXiv",
   eprint = {0706.3732},
 primaryClass = "gr-qc",
 keywords = {Post-Newtonian approximation, perturbation theory, related approximations, Numerical relativity, Wave generation and sources, Classical black holes},
     year = 2007,
    month = nov,
   volume = 76,
   number = 10,
      eid = {104049},
    pages = {104049},
      doi = {10.1103/PhysRevD.76.104049},
   adsurl = {http://adsabs.harvard.edu/abs/2007PhRvD..76j4049B},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

