#+COLUMNS: %50ITEM %custom_id

/This chapter covers the introductory theory of Gaussian process regression, and the development of the ~Heron~ model for binary black hole coalescence waveforms./

* Surrogate modelling

Many experimental scenarios in science and engineering are expensive, laborious, or both to perform, and therefore difficult or off-putting to repeat.
The ability to perform such an experiment with a specific set of parameters may however be valuable.

While in engineering the difficulties of performing repeatative experiments may relate to the nature of the experiment (which might, for example, be destructive to a tested material), in astrophysics we are more frequently hampered by the computational expense of running a numerical simulation.

A number of techniques exist to perform interpolation between experimentally measured data. 
If a well-defined physical theory is known to explain the observations an appropriate function can be derived to make predictions outwith the sampled parameter space for the experiment, and this can be calibrated (fitted) to the data.

Surrogate models are often desirable in situations where little physical intuition is available (or where the underlying physics is extremely complicated).
These attempt to model the observed data with limited assumptions about the form of the generating function.

** Polynomial response surfaces



** Support vector machines
** Gaussian process regression
** Artificial neural networks
** Bayesian networks

* From Bayesian linear regression to Gaussian Process
  :PROPERTIES:
  :CUSTOM_ID: sec:gpr:gpr-from-blr
  :END:

This choice of a Gaussian prior also implies that $y_i$ will have a
Gaussian distribution, and we can take this to have the form
$$\vec{y} \sim \mathcal{G}(\vec{0}, \mat{C})$$ where $\mat{C}$ is the
/covariance matrix/, or /gram matrix/, which describes the covariance of the input data, as
defined by some /covariance function/, or /kernel/, $K$,

#+NAME:eq:covariance-matrix-derivation
\begin{aligned}
  C_{ij} &= K(\vec{x_i}, \vec{x_j}) = \ex(y_i y_j) = \ex(\vec{x}_i \vdot \vec{w} \vec{w} \vdot \vec{x}_j) + \ex(\epsilon_i \epsilon_j) \\
&= \vec{x}_i^T \ex(\vec{w} \vec{w}^T) \vec{x}_j  + \ex(\epsilon_i \epsilon_j) \\&= \sigma_w^2 \vec{x}_i^T \vec{x}_j + \delta_{ij} \sigma_\epsilon^2,
\end{aligned}

for $\ex(x)$ the expectation of a variable $x$. As a result of this
relationship between the weight vector, $\vec{w}$ and the gram matrix it
is possible to perform the regression by means of a covariance function,
rather than inferring the values $w_i$, and this is the justification by
which Gaussian Process Regression (GPR) is often deemed a
"non-parameteric" regression model[fn:parametric].


[fn:parametric] This claim is rather sketchy, as we'll see when the forms of
    covariance function are presented, as the parametricity is simply
    moved from the model itself to the form of the covariance functions,
    and the values of these /hyperparameters/ must be inferred, or
    learned, from the data.

* Gaussian Processes
  :PROPERTIES:
  :CUSTOM_ID: sec:gpr:gp
  :END:

#+CAPTION: A graphical model of a Gaussian process, represented as a chain graph. The inputs (on the bottom row) are all observed quantities, while outputs are observed only at the location of training points. The latent variables, $f$ from the Gaussian field (the heavy black line connecting these nodes indicates that they are fully connected) connect the two, and so any given observation is independent of all other nodes given it connected latent $f$ variable. Thus the marginalisation (removal) or addition of input nodes to the abbr:gp does not change the distribution of the other variables.
#+NAME: fig:gp:chain-diagram
#+BEGIN_figure
\begin{center}
\begin{tikzpicture}

	 \node[obs] (x1) {$\vec{x}_{1}$};	 	
	 \node[latent, above = of x1] (f1) {$f_{1}$};
	 \node[obs, above = of f1] (y1) {$y_{1}$};
	 \edge{x1}{f1};
	 \edge{f1}{y1};

	 \node[obs, right = of x1] (x2) {$\vec{x}_{2}$};	 	
	 \node[latent, above = of x2] (f2) {$f_{2}$};
	 \node[obs, above = of f2] (y2) {$y_{2}$};
	 \edge{x2}{f2};
	 \edge{f2}{y2};

	 \node[obs, right = of x2] (xstar) {$\vec{x}_{\star}$};	 	
	 \node[latent, above = of xstar] (fstar) {$f_{\star}$};
	 \node[latent, above = of fstar] (ystar) {$y_{\star}$};
	 \edge{xstar}{fstar};
	 \edge{fstar}{ystar};

	 \node[obs, right = 2 of xstar] (xN) {$\vec{x}_{N}$};	 	
	 \node[latent, above = of xN] (fN) {$f_{N}$};
	 \node[obs, above = of fN] (yN) {$y_{N}$};
	 \edge{xN}{fN};
	 \edge{fN}{yN};

	 \draw [black, line width=0.1cm] (f1) -- (f2) -- (fstar);
	 \draw [black, dashed, line width=0.1cm] (fstar) -- (fN);
\end{tikzpicture}
\end{center}
#+END_figure


Gaussian processes are Bayesian models which associate every point an some /input space/ with a probability distribution---specifically a normal distribution, and a collection of input points will form a multi-variate normal distribution. 
Gaussian processes are a non-parametric supervised machine learning technique\cite{barberBRML2012,2003itil.book.....M}.

To make a prediction, we need to first have a set of prior observations and information about the parameters of the physical system which produced those observations. 
These combined form the /training data/ for the predictor.

#+LATEX_ATTR: :options [Gaussian process]
#+BEGIN_definition
A gls:gaussian-process is a collection of random variables, any finite number of which have a joint Gaussian distribution cite:gpr.book.rw.
#+END_definition

A completely untrained Gaussian process forms the job of a prior probability distribution in a Bayesian analysis; where it is more conventional to consider a prior over a set of, for example, real values, such as a normal distribution, the Gaussian process forms a prior over the functions which might form the regression fit to any observed data. 
While this prior is intially untrained it still contains information about our preconceptions of the data, for example, whether or not we expect the fit to be analytic, and the average of the functions. 
By providing training data we can update the Gaussian process, in the same way that the posterior distribution is updated by
the addition of new data in a standard Bayesian context, and a posterior on the set of all possible functions to fit the data is produced. 
We can take the mean of this posterior in the place of the "best fit line" which other techniques produce, and then use the variance to produce an estimate of the uncertainty of the prediction.

# The possibility of using Gaussian Processes in the analysis of data from
# gravitational wave detectors has been proposed by Moore and
# Gair\cite{2014PhRvL.113y1101M,2016PhRvD..93f4001M} who propose its use
# to calculate the uncertainties in current generation post-Newtonian
# approximants to numerical relativity simulations, and to incorporate
# this into the current matched-filtering analyses which are conducted on
# triggers from gravitational wave detectors.

Gaussian processes trained with $N$ data require the ability to both store and invert an $N\times N$ matrix of covariances between observations; this can be a considerable computational challenge.
There are a number of approaches to get around this problem, including /sparse Gaussian processes/, where a limit on the parameter-space distance between training points is set, and the covariance of points outside this radius are ignored\cite{EPFL-CONF-161319}, and hierarchical matrix inversion methods\cite{hodlr}.

Gaussian processes can be extended from the case of a single-dimensional input predicting a single-dimensional output to the ability to predict a multi-dimensional output from a multi-dimensional input\cite{Alvarez2011,Alvarez2011a,Bonilla2007}.

#+CAPTION: [Step 1] An example of raw training data which is suitable for training a Gaussian process. In this example the input data ($x$-axis) are 1-dimensional, although GPs are also capable of handling multi-dimensional data.
#+NAME: fig:gp-training-data
#+ATTR_LATEX: :width \textwidth
file:figures/gp-training-data.pdf

#+CAPTION: [Step 2] We choose a covariance function for the  Gaussian process, in this case an exponential-squared covariance    function. The Gaussian process containing no data and this    covariance matrix forms our prior probability distribution. Here    50 draws from the prior distribution are plotted.
#+LABEL: fig:gp-prior
#+ATTR_LATEX: :width \textwidth
file:figures/gp-example-prior-draws.pdf

#+CAPTION: [Step 3] The trained Gaussian process can be     sampled multiple times to produce multiple different potential     fitting functions. Here 50 draws from the Gaussian process posterior are    displayed.}
#+LABEL: fig:gp-covariance-matrix
#+ATTR_LATEX: :width \textwidth
file:figures/gp-example-posterior-draws.pdf

# #+CAPTION: The covariance structure for the Gaussian process.
# #+LABEL: fig:gp-covariance-matrix
# #+BEGIN_figure
#   \includegraphics{figures/gp-example-expsqcov-matrix.pdf}
# #+END_figure

#+CAPTION: [Step 4] We can then take the mean and the covariance of the Gaussian process, and produce a single ``best-fit'' with confidence intervals.
#+LABEL: fig-gp-posterior-best
#+ATTR_LATEX: :width \textwidth
file:figures/gp-posterior-meancovar.pdf


* Covariance Functions
  :PROPERTIES:
  :CUSTOM_ID: sec:gpr:covariance
  :END:

The covariance function defines the similarity of a pair of data points, according to some relationship with suitable properties. 
The similarity of input data is assumed to be related to the similarity of the output, and therefore the more similar two inputs are the more likely their outputs are to be similar.

As such, the form of the covariance function represents prior knowledge about the data, and can encode understanding of effects such as periodicity within the data.

#+ATTR_LATEX: :options [Stationary covariance function]
#+BEGIN_definition
A stationary covariance function is a function $f(\vec{x} - \vec{x}')$, and which is thus invariant to translations in the input space.
#+END_definition

#+ATTR_LATEX: :options [Isotropic Covariance Function]
#+BEGIN_definition
If a covariance function is a function of the form $f(|\vec{x} - \vec{x}'|)$ then it is isotropic, and invariant under all rigid motions.
#+END_definition

A covariance function which is both stationary and isotropic has the property that it can be expressed as a function of a single variable, $r = | \vec{x} - \vec{x}' |$ is known as a abbr:rbf.
Functions of the form $k : (\vec{x}, \vec{x}') \to \mathbb{C}$, for two vectors $\vec{x}, \vec{x}' \in \mathcal{X}$ are often known as /kernels/, and I will frequently refer interchangably to covariance functions and kernels where the covariance function has this form.

For a set of points $\setbuilder{ \vec{x}_{i} | i = 1, \dots, n }$ a kernel, $k$ can be used to construct the gram matrix, $K_{i,j} = k(x_{i}, x_{j})$.
If the kernel is also a covariance function then $K$ is known as a /covariance matrix/.

For a kernel to be a valid covariance function for a abbr:gp it must produce a positive semidefinite covariance matrix $K$. 
Such a matrix, $K \in \mathbb{R}^{n \times n}$ must satisfy $\vec{x}^{\transpose} K \vec{x} \geq 0 \forall \vec{x} \in \mathbb{R}^{n}$.


** Example covariance functions




One of the most frequently encountered covariance functions in the literature is the abbr:se covariance functions cite:gpr.book.rw.
Perhaps as a result of its near-ubiquity this kernel is known under a number of similar, but confusing names (which are often inaccurate).
These include the /exponential quadratic/, /quadratic exponential/, /squared exponential/, and even /Gaussian/ covariance function.

The reason for this is its form, which closely resembles that of the Gaussian function:

#+NAME: eq:gp:kernels:se
\begin{equation}
   \label{eq:gp:kernels:se}
  k_{\mathrm{SE}}(r) = \exp \left( - \frac{r^2}{2 l^2} \right)
\end{equation}

for $r$ the Euclidean distance of a datum from the centre of the parameter space, and $l$ is a scale factor associated with the axis along which the data are defined.

#+CAPTION: The *squared exponential* covariance function (defined in equation ref:eq:gp:kernels:se). The panel on the left depicts the value of the kernel as a function of $r = (|\vec{x} - \vec{x}'|)$, at a number of different length scales ($l = 0.25, 0.5, 1.0$) while the panel on the right contains draws from Gaussian processes with abbr:se covariance with the same length scales as the left panel.
#+LABEL: fig:gp-expsq-covar
#+ATTR_LATEX: :width \textwidth
file:figures/gp/covariance-se-overview.pdf

The squared exponential function imposes strong smoothness constraints on the model, as it is infinitely differentiable.

The scale factor, $l$ in ref:eq:gp:kernels:se, also known as its /scale-length/ defines the size of the effect within the process. 
This characteristic length-scale can be undnerstood cite:adler1976,gpr.book.rw in terms of the number of times the abbr:gp should cross some given level (for example, zero).
Indeed, for a abbr:gp with a covariance function $k$ which has well-defined first and second derivatives the expected number of times, $N_{u}$ the process will cross a value $u$ is 

\begin{equation}
\label{eq:gp:kernels:crossings}
\mathbb{E}(Nᵤ) = \frac{1}{2 \pi} \sqrt{ - \frac{ k''(0) }{k(0)} } \exp \left( - \frac{u²}{2k(0)} \right)
\end{equation} 

A zero-mean abbr:gp which has an abbr:se covariance structure will then cross zero $1/(2 \pi l)$ times on average.

#+CAPTION: The *squared exponential* covariance function (defined in equation ref:eq:gp:kernels:se). The panel on the left depicts the value of the kernel as a function of $r = (|\vec{x} - \vec{x}'|)$, at a number of different length scales ($l = 0.25, 0.5, 1.0$) while the panel on the right contains draws from Gaussian processes with abbr:se covariance with the same length scales as the left panel.
#+LABEL: fig:gp-exp-covar
#+ATTR_LATEX: :width \textwidth
file:figures/gp/covariance-ex-overview.pdf

#+CAPTION: The *squared exponential* covariance function (defined in equation ref:eq:gp:kernels:se). The panel on the left depicts the value of the kernel as a function of $r = (|\vec{x} - \vec{x}'|)$, at a number of different length scales ($l = 0.25, 0.5, 1.0$) while the panel on the right contains draws from Gaussian processes with abbr:se covariance with the same length scales as the left panel.
#+LABEL: fig:gp-m32-covar
#+ATTR_LATEX: :width \textwidth
file:figures/gp/covariance-mat32-overview.pdf

#+CAPTION: The *squared exponential* covariance function (defined in equation ref:eq:gp:kernels:se). The panel on the left depicts the value of the kernel as a function of $r = (|\vec{x} - \vec{x}'|)$, at a number of different length scales ($l = 0.25, 0.5, 1.0$) while the panel on the right contains draws from Gaussian processes with abbr:se covariance with the same length scales as the left panel.
#+LABEL: fig:gp-m52-covar
#+ATTR_LATEX: :width \textwidth
file:figures/gp/covariance-mat52-overview.pdf

#+CAPTION: The *squared exponential* covariance function (defined in equation ref:eq:gp:kernels:se). The panel on the left depicts the value of the kernel as a function of $r = (|\vec{x} - \vec{x}'|)$, at a number of different length scales ($l = 0.25, 0.5, 1.0$) while the panel on the right contains draws from Gaussian processes with abbr:se covariance with the same length scales as the left panel.
#+LABEL: fig:gp-rq-covar
#+ATTR_LATEX: :width \textwidth
file:figures/gp/covariance-rq-overview.pdf

** Kernel algebra

It is possible to define new kernels from the standard set through a
series of defined operations.

Consider two covariance functions, $f_1$ and $f_2$, then

#+ATTR_LATEX: :options [Kernel Addition]
#+BEGIN_definition
If $f₁$ and $f₂$ are both kernels, then 
$f = f_1 + f_2$ is also a kernel.
#+END_definition

#+ATTR_LATEX: :options [Kernel Multiplication]
#+BEGIN_definition
If $f₁$ and $f₂$ are both kernels, then 
$f = f_1 × f_2$ is also a kernel.
#+END_definition

We can think of the sum of two kernels as representing the possibility that the data be described by one component kernel or another.
As such addition represents the logical OR operation. 
Similarly the product of two kernels represents the logical AND operation between the two.

We can use these two operations to form an arbitrarily complicated kernel structure, and to allow inference to be conducted over multiple dimensions.
Different kernels can be used to model different aspects of the variation within the input data. 
For example, the training data may be known to be periodic in one dimension, or to have white noise properties in another. 
Here I adopt the convention from cite:duvenaud.thesis.2014 and omit the hyperparameters from the description of the kernel.
I also extend the notation to allow kernels with multiple input dimensions to be described, with superscript indices indicating the dimensions of the training data which the kernel applies to.

As a concrete example, for a kernel function in which the zeroth dimension is described by a squared-exponential kernel, but the first, second, and third dimensions are described by a rational quadratic kernel the kernel could be described as

\begin{equation}
\label{eq:example-kernel-notation}
k = \SE^{(0)} \times \RQ^{(1,2,3)}
\end{equation}

A list of the symbols for each covariance function is given in table ref:tab:kernels, and definitions of the kernels are given at the end of the chapter.


#+ATTR_LATEX: :environment tabularx 
#+ATTR_LATEX: :width \textwidth
#+ATTR_LATEX: :align lcX :booktabs
#+ATTR_LATEX: :placement [b]
#+CAPTION: A table of commonly encountered covariance functions.
#+NAME: tab:kernels
#+LABEL: tab:kernels
#+tblname: tab:kernels
| Kernel              | Sym.     | Properties                          |
|---------------------+----------+-------------------------------------|
| Squared-exponential | $\SE$    | Smooth local variation.             |
| Matern-3/2          | $\M32$   |                                     |
| Matern-5/2          | $\M52$ |                                     |
| Periodic            | $\Per$   | Smooth global periodic variation.   |
| Linear              | $\Lin$   | Global continuous linear variation. |
| Rational Quadratic  | $\RQ$    | Variation on multiple scales.       |
| Constant            | $\Con$   | Scaling factor.                     |

For example, we may be able to model a yearly growing trend which contains a seasonal variation with a combination of a linear and a
periodic kernel, $\Lin \times \SE$.


** Higher-dimensional problems


* Training the model
  :PROPERTIES:
  :CUSTOM_ID: sec:gpr:training
  :END:

When defining the covariance function for the it may be desirable to
specify a number of free hyperparameters, $\theta$, which allow the
properties of the GP to be altered, effectively allowing Bayesian model
comparison to be carried-out to select the Gaussian Process which
optimally describes the data. The log-probability that a given set of
strain values were drawn from a Gaussian process with zero mean and a
covariance matrix $K_{ij} = k(x, x')$ is

\begin{equation}
\label{eq:logevidencegp}
  \log(p(\vec{f}| X)) = - \frac{1}{2} K^{-1} \vec{f} - \frac{1}{2} \log |K| - \frac{n}{2} \log 2\pi.
\end{equation}

This quantity is normally denoted the /log-evidence/ or the
/log-hyperlikelihood/. The model which best describes the training data
may then be found by maximising the log-hyperlikelihood with respect to
the hyperparameters, $\theta$ of the covariance function.

This optimisation may be conducted using either a hill-climbing based
optimisation algorithm, or in a hierarchical Bayesian framework, whereby
priors are assigned to the value of each hyperparameter, and the optimal
hyperparameters are found using a Monte Carlo algorithm.

* The predictive posterior distribution
  :PROPERTIES:
  :CUSTOM_ID: sec:gpr:predictive
  :END:

In order to make a prediction using the Gaussian Process model we
require a new input at which the prediction should be made, which is
denoted $x^*$. In order to form the predictive distribution we must then
calculate the covariance of the new input with the existing training
data, which we denote $K_{x, x^*}$, and the autocovariance of the input,
$K_{x^*, x^*}$. We then define a new covariance matrix, $K^{+}$, which
has the block structure

\begin{equation}
\label{eq:blockK-plus-mat}
  K^+ =
  \begin{bmatrix}
    K_{x,x} & K_{x,x^*} \\ K_{x^*,x} & K_{x^*, x^*}
  \end{bmatrix}
\end{equation}

for $K_{x,x}$ the covariance matrix of the training inputs, and
$K_{x^*,x} = K_{x,x^*}^T$.

The predictive distribution can then be found as

\begin{equation}
\label{eq:predictive-gp}
  p(y^* | x^*, \mathcal{D}) = \mathcal{N}(y^* | K_{x^*,x} K_{x,x}^{-1} y, K_{x^*, x^*} - K_{x^*,x}K^{-1}_{x,x} K_{x,x^*}).
\end{equation}

* Computational Complexity
  :PROPERTIES:
  :CUSTOM_ID: sec:gpr:complexity
  :END:

One severe disadvantage of Gaussian Processes as a data analysis tool
are their high computational complexity. Producing a prediction from a
GP requires inverting the covariance matrix; matrix inversion is an
$\mathcal{O}(N^3)$ process in time, and scales with $\mathcal{O}(N^2)$
in memory use. This effectively limits the number of training points
which can be input to a GP to fewer than $10^4$. A number of approaches
have been developed in the literature to address this short-coming by
utilising computationally tractable approximations to either the matrix
inversion or the Gaussian process.

These approaches can be grouped into three broad categories; sparse
Gaussian processes, which use a modified covariance function to force
the covariance matrix to have a near-diagonal structure; hierarchical
approaches, which do not modify the covariancec function, but
approximate the off-diagonal terms' influence on the inversion; and
local expert approaches, in which the parameter space is divided into
many sub-spaces, and each sub-space is modelled using an independent
Gaussian process.

** Sparse Gaussian proceses

** Hierarchical Gaussian processes

** Gaussian process local experts

Local expert approaches attempt to improve the computational performance
of GPs by diving the parameter space of the model into multiple
sub-spaces. In a conventional GP the training data,
$\mathcal{D} = \{ (x^n, y^n), n=1,\dots,N \} = \mathcal{X} \cup \mathcal{Y}$, 
is used in its entirity to train a single GP. If these
data were instead divided into $M$ subsets, of size $K$, we can train
$M$ separate GPs, which will each provide an independent prediction for
any given point in the parameter space. The network structure which is
established by this subdivision of the parameter space is known as a
/gating network/.

Early approaches to using local experts in GPs used kd-trees cite:shen2005fast to sub-divide the parameter space, and then modelled each subspace with its own GP. 
The GPs were trained together, with each having the same kernel hyper-parameters. 
Final predictions were then produced as a weighted sum of the individual GPs' predictions. 
While this approach was somewhat effective, it enforced a stationary structure on the covariance matrix, and the paper does not treat the combination of the prediction uncertainties.

Approaches which follow the work of cite:Jacobs:1991:AML:1351011.1351018 on mixtures of local experts have had some more promise, allowing each GP to have its own set of hyper-parameters, allowing greater freedom in modelling heteroskedastic and non-stationary data.

Deciding on the number of sub-models is a non-trivial problem; one
approach is to model the parameter space using an abbr:imm cite:rasmussen2002infinite, in which the gating network is effectively a Dirichlet process over the training data. 
The predictions from each sub-model are then summed to find the global prediction. 
While this approach offers greater flexibility for modelling more complex underlying functions, it does little to improve the speed of GP predictions. 
Additional abbr:imm approaches are proposed by cite:meeds2006alternative, and a comparable, variational approach is taken by cite:yuan2009variational.

All of these approaches have the difficulty of requiring the gating network to assign a weight (often called a /responsibility/ to each sub-model's prediction when calculating the global prediction, adding an
additional layer of inference, which normally requires an MCMC sampler to perform. 
/Product-of-experts/ models avoid this complication by multiplying the sub-model predictions, but these models have either turned out to be excessively confident,\cite{2014arXiv1412.3078N}, or excessively conservative\cite{2014arXiv1410.7827C}.

These problems have lead to the development of the Bayesian Committee Machine (BCM)\cite{tresp2000bayesian}, which assigns a weight to each sub-model's prediction which is equal to the inverse of the prediction's covariance, in order that sub-models which better observe the predicted region are given a greater weight in the global prediction. 
This approach can suffer as a result of models which contains week experts, and so the /robust Bayesian Committee Machine/\cite{deisenroth2015distributed} has been proposed to provide a more robust framework for Gaussian process regression with many experts.
This approach also allows for the computation of the model's prediction to be highly-parallelised, with the potential for each sub-model being evaluated on separate compute nodes, and combined together by another process running on another node.

** Stochastic Variational Inference

* Assessing the model
  :PROPERTIES:
  :CUSTOM_ID: sec:gpr:assessing
  :END:

Having produced a statistical regression model it is crucial that its efficacy is assessed.
There are broadly two scenarios under which such testing can occur. 
In situations where a large amount of data is available to condition the model it is often appropriate to partition the data into a "training set" and a "test set"; 
the latter is held-aside, and not used to condition the model, and can then be used after the model is trained to compare against the model predictions.

Alternatively scenarios may arise where there is insufficient data to form such a test set without adversely affecting the model's predictive power.
Examples of such a scenario include timeseries modelling, where the predictions of the model may represent future (an therefore inaccessible) observations, 
or computational experiments, where the acquisition of training data is sufficiently costly that producing a test set is not viable.

In the case where test data is available two straight-forward metrics are available: the root-mean-squared error, and the correlation. 

Let $\vec{x}_*$ and $\vec{y}_*$ be respectively the test inputs and test outputs from the test set, 
then let $\hat{y}$ be the set of model predictions drawn from the Gaussian Process with inputs $\vec{x}_*$.

The root mean squared error (RMSE) gives an estimate of the total deviation between the mean prediction of the model and the true value from the test data:
#+NAME:eq:rmse
\begin{equation}
\mathrm{RMSE} = \sqrt{
    \frac{
      \sum_{i=0}^{n_i} (y_*^{(i)} - \hat{y}^{(i)})^2
    }
    { n_t },
  }
\end{equation}

for $n_t$ the size of the test set. While the RMSE can represent a good metric for conventional regression methods, it does not consider the estimate of the variance which is provided by Gaussian process models; 
as such it is an insufficient measure on its own of these models.

It is possible to use the Gaussian process variance to form a metric of the efficacy by considering the correlation between the test data and the prediction

#+NAME:eq:correlation
\begin{equation}
    \rho^2 = \left(
      \frac{ \cov(y^*, \hat{y})} { \sqrt{ \vary(y) \vary(\hat{y}) } } 
    \right)^2
\end{equation}

These two metrics, together, allow the model to be assessed either during the training of the model 
(or indeed, they can be used as training metrics if using a cross validation approach while determining the model hyperparameters)
given a judicious partitioning of the available data.

Forrester\cite{forrester2008engineering} suggests that a $\rho^2 \geq 0.8$ provides a surrogate model with good global predictive abilities, which corresponds to an RMSE of around $0.1$.

In situations where test data is not available such straightforward tests are often impractical. 
In the case of timeseries forecasting it may be possible to assess the forecast by forming a test set from the most recent observations, and comparing these to the output of the model, 
however, if only a small number of past observations are available the predictive capability of the model may be sufficiently poor to render this test almost meaningless.


* Bayesian Optimisation
  :PROPERTIES:
  :CUSTOM_ID: sec:gpr:optimisation
  :END:

While conventional optimisation methods, such as hill-climbing
algorithms, rely on the ability to evaluate a function (and often its
derivative) locally, the existence of a surrogate model allows optima to
be found using the entire structure of the function as part of a
Bayesian framework.

** Acquisition Functions

When using our Gaussian Process as a surrogate model to the underlying
generative model for the waveform we treat the function which generates
waveforms as unknown, and we place a prior on it, and the training data
is used to update the prior, providing a posterior. We may use the
posterior to determine the appropriate location for future evaluations
from the underlying model; an infill sampling criterion, or acquisition
function. This approach of using a surrogate model to approximate an
underlying function which is hard or costly to evaluate is treated in
the discipline of /Bayesian optimisation/.

Increasing the accuracy of the surrogate to the underlying function can
be achieved by sampling the function at various points through parameter
space, however, a strategy for performing this in an optimal manner is
desirable, given the properties of that function. For example, if one
were attempting to find which combination of components in concrete
produced the strongest building product one might require a lengthy
period to allow it to set, and so minimising the number of sampling
iterations is desirable. We define an acquisition function, $f$, such
that for a desirable new sample, $x^+$,

$$\label{eq:acquisition}
  x^+ = \mathrm{argmax} f(x)$$

** Probability of Improvement

One possible acquisition function considers the probability that a
sampled point improves the model, suggested first in \cite{Kushner1964},

$$\label{eq:probabilityimprovement}
  \mathrm{PI}(x) = P(f(x) \geq f(x^+)) = \mathrm{CDF}\left(\frac{\mu(x) - f(x^+)}{\sigma(x)} \right)$$

This algorithm clearly attempts to /exploit/ the parameter space, that
is, it samples areas only where the greatest improvement over the
current observation are possible. In order to force /exploration/ of the
parameter space---sampling areas of high uncertainity---a trade-off
parameter, $\xi\geq 0$ may be instroduced, such that

$$\label{eq:probabilityimprovementexplore}
    \mathrm{PI}(x) = P(f(x) \geq f(x^+) + \xi) = \mathrm{CDF}\left(\frac{\mu(x) - f(x^+) - \xi}{\sigma(x)} \right)$$

\cite{Kushner1964} suggests that this should be varied according to some
pre-defined schedule, tending to 0 as the algorithm runs.

** Expected Improvement

In order to address the arbitrary nature of the choice of $\xi$ in the
Probability of Improvement function we may consider not only the
probability that a point provides an improvement, but also the magnitude
of that improvement. In this situation we wish to minimise the expected
deviation from the true $f(x^+)$ when choosing a trial point, so

** Entropy Search

** Upper confidence bound

# ** Waveform Match

# The match between two waveforms, $A$, and $B$, is defined as

# $$\label{eq:waveformmatch}
#   \mathcal{N} = \frac{
#     \max\limits_{t_0, \phi_0} \left< A , B \right>
#     }
#     {
#       \left< A, A\right>^{\half}
#       \left< B, B\right>^{\half}
#     }$$

# for the initial time and phase respectively $t_0$ and $\phi_0$.

# Suppose we wish to compare the surrogate model to an alternative
# approximant, for example, =IMRPhenomP=, and identifying the location in
# parameter space where the two have the greatest disagreement. This can
# be achieved by finding the location in the parameter space of the
# surrogate which has the minimum match to the alternative model.

* Examples
  :PROPERTIES:
  :CUSTOM_ID: sec:gpr:examples
  :END:

** A single BBH waveform

A trivial task is to reproduce a waveform from a Gaussian Process which
is trained on a single waveform which is generated at one set of
parameters.

# the script which is used for this section can be found in scripts/gp/single-waveform.py
# this file is also available as an iPython notebook.

#+CAPTION: The properties of the training waveform used for the model.
#+NAME: tab:imrphenomparamssingle
| Property         | Value                                       |
|------------------+---------------------------------------------|
| Mass (Primary)   | 5                                           |
| Mass (Secondary) | 6                                           |
| Spin (Primary)   | $(0,0,0)$                                   |
| Spin (Secondary) | $(0,0,0)$                                   |
| Distance         | $\SI{400}{\mega\parsec}$                    |
| Time range       | $(\SI{-0.1}{\second}, \SI{0.005}{\second})$ |

#+CAPTION: A Gaussian process trained on data from a single abbr:bbh gravitational waveform.
#+ATTR_LATEX: :width \textwidth
file:figures/gp/single-waveform.pdf


As a first test we generated a BBH waveform using the model, as
implemented in the package. The model was evaluated at the parameters
listed in table ref:tab:imrphenomparamssingle, and 300 equally-spaced
points from the evaluation were used to train a Gaussian process, using
an exponential squared covariance function with a constant
pre-multiplier. The model was trained using the BFGS algorithm (a
Newtonian-like hill-climbing optimiser), which was provided with initial
values determined according to Jaakkola's heuristic [2]. The samples
were around $\SI{0.003}{\second}$ separated along the time dimension,
and so the initial value of $\lambda_{\text{time}} = 300$ was selected.
An initial value for the constant term in the kernel was slected from
the data's variance. Following optimisation the values
$$\lambda_{\text{amp}} = 26.8, \qquad \lambda_{\text{time}} =
    111.6$$ were found to minimise the log-likelihood of the model. The
trained model was tested against a set of data generated by at the same
parameter values, but with 1000 samples in time rather than 300. In
order to test the global accuracy of the model the correlation and RMSE
were calculated, with $$\rho^2 = 0.90, \qquad \rmse = 8.22\e{-23}$$

[ref:fig:simplewaveform1]

** Estimating contours

   #+LABEL: fig:gp:examples:mountains1
   #+ATTR_LATEX: :width \textwidth
   #+CAPTION: Trained off a small number of spot-heights, a abbr:gp is capable of estimating the landscape surrounding those points. This plot depicts the mean abbr:gp output for a abbr:gp trained with summit heights in the Arrochar Alps, an upland area north of Glasgow, Scotland. Here the smoothness conditions placed on the abbr:gp by the form of the covariance function become clear with a number of the peaks being lost as a result. In this example a rational-quadratic covariance function was used.
   file:figures/gp/arrochar-alps.pdf

   #+LABEL: fig:gp:examples:mountains1
   #+ATTR_LATEX: :width \textwidth
   #+CAPTION: The abbr:gp derived landscape, with a variety of different covariance functions used to produce the interpolated topology. 
   file:figures/gp/arrochar-kernels.pdf

** A concrete example


* Gaussian processes and experimental design
  :PROPERTIES:
  :CUSTOM_ID: sec:gpr:design
  :END:

** Preparing training data

   #+CAPTION: The Rosenbrock saddle function, a standard function used to test numerical optimisation algorithms.
   #+ATTR_LATEX: :width \textwidth
   file:figures/gp/rosenbrock-function.pdf

   #+CAPTION: Twenty-five randomly selected samples from the Rosenbrock function.
   #+LABEL: fig:gp:design:initial:rosen:random:training
   #+ATTR_LATEX: :width \textwidth
   file:figures/gp/rosen-random-training.pdf

   #+CAPTION: The output of a abbr:gp trained on the 25 samples from figure ref:fig:gp:design:initial:rosen:random:training 
   #+ATTR_LATEX: :width \textwidth
   file:figures/gp/rosen-random-trained-25.pdf

   #+CAPTION: The output of abbr:gp models trained with an increasing number of samples from the Rosenbrock function at randomly selected locations within the function's parameter space, with the left panel representing the mean prediction of the abbr:gp and the right panel the standard deviation across the domain of the function.
   #+ATTR_LATEX: :width \textwidth
   file:figures/gp/rosen-random-progress.pdf

   #+CAPTION: The output of abbr:gp models trained with an increasing number of samples from the Rosenbrock function at locations defined by a full factorial sampling plan within the function's parameter space, with the left panel representing the mean prediction of the abbr:gp and the right panel the standard deviation across the domain of the function.
   #+ATTR_LATEX: :width \textwidth
   file:figures/gp/rosen-factorial-progress.pdf

   #+CAPTION: The output of abbr:gp models trained with an increasing number of samples from the Rosenbrock function at locations defined by a latin squares sampling plan within the function's parameter space, with the left panel representing the mean prediction of the abbr:gp and the right panel the standard deviation across the domain of the function.
   #+ATTR_LATEX: :width \textwidth
   file:figures/gp/rosen-latin-progress.pdf

** Model infill

   #+CAPTION: A abbr:gp surrogate for the Rosenbrock saddle function, with the mean prediction in the left panel and the standard deviation of that prediction on the right. The 25 training points, which were devised using a latin hypercube sampling plan, are plotted as circles coloured according to the true value of the function at those points in the left plot, and as crosses on the right plot. A candidate location for a new sample is plotted as a blue circle on both plots. This point represents the location where the model has produces the largest variance in its prediction.
   #+ATTR_LATEX: :width \textwidth
   [[file:figures/gp/infill-max-uncertainty.pdf]]
   

** Conventional sample planning methods
*** Latin hypercubes
   :PROPERTIES:
   :CUSTOM_ID: sec:gpr:design:hypercubes
   :END:
** Voronoi tesselation
   :PROPERTIES:
   :CUSTOM_ID: sec:gpr:design:voronoi
   :END:

* Extending the Gaussian Process
  :PROPERTIES:
  :CUSTOM_ID: sec:gpr:extending
  :END:

Standard implementations of Gaussian processes are capable of mapping a multi-dimensional input to a single-dimensional output, however there are many situations in which the ability to generate a multi-dimensional output would be advantageous.

* General elliptical processes
  :PROPERTIES:
  :CUSTOM_ID: sec:gpr:elliptical
  :END:
/This section should be a discussion of more general elliptical processes, such as student-t processes. Given that these haven't featured in the development of the surrogate model it would be appropriate to only spend a few lines on this, although finding and presenting a suitable astrophysical example could be valuable./
