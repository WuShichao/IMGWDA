#+TITLE: Probability

* Probability 
In this work we consider "probability" as the notion of our
degree-of-belief in some event or occurence. For example, a coin, when
tossed, can take one of two states when it lands, which we call
"heads" or "tails"; we denote these two states $H$ and $T$. Assuming
that we have no knowledge of a reason why the coin would fall one way
rather than the other, we are forced to conclude that we have no more
reason to believe that the coin will fall heads, compared to it
falling tails.

With this philosophy, we can construct a mathematical model of
probability. We make some additional demands on this model, one of
which is that an event which is considered "certain" should have a
probability of $1$, maintaining consistency with boolean
logic. Likewise, if we are certain that an event will not occur, then
we assign it a value $0$. 

#+ATTR_LATEX: :options [Sample Space]
#+BEGIN_definition
If a variable can take on a number of
different values, then the set of all its possible values is called
the sample space. In any given problem, the sample space composes
the universe set, and is often denoted $\Omega$.
#+END_definition

#+ATTR_LATEX: :options [State]
#+BEGIN_definition
A state (or event) is any subset of zero or   more elements from the sample space. A state containing one element  is a simple state, while one containing more than one element is a compound state. The states form a $\sigma$-algebra on the sample space.
#+END_definition

#+ATTR_LATEX: :options [Probability]
#+BEGIN_definition
Let $x$ be some variable which is capable
  of having a state $E \in \Omega$ for $\Omega$ the /sample space/ of
  the variable. Probability $P$ is a mapping $P: E \to [0,1]$ which
  assigns a real value between $0$ and $1$ to every $E \in \Omega$. We
  place three constraints on the form of this mapping:
  1. For every $E \in \Omega$, $P(E) \geq 0$.
  2. $P(\Omega) = \sum_{E \in \Omega} P(E) = 1$.
  3. If $E_{i} \cap E_{j} = \emptyset$ for $E_{i}, E_{j} \in \Omega$, then $P(\cup E_{i}) = \sum P(E_{i})$
#+END_definition

#+ATTR_LATEX: :options [Joint probability]
#+BEGIN_definition
The probability of two events, $A,B
  \in \Omega$, occuring is a "joint probability", and is computed as \[ P(A \cap B) = P(A, B) = P(A) P(B). \]
#+END_definition

+ *Corollary*: For two events, $A,B \in \Omega$, we have $P(A \cup B) = P(A) + P(B) - P(A \cap B)$

In the case that we have two events which occur with some dependence
between them, we can form a "conditional probability", for example, if
there can be no smoke without fire, then the probability of smoke can
be conditional on the probability of fire.

+ *Definition*: Conditional probability. Given two events, $A,B \in
  \Omega$, then the probability of $A$ /given/ $B$ is \[ P(A | B) =
  \frac{ P(A,B) }{ p(B) }. \] If $P(B) = 0$ then $P(A)$ is undefined.

Given that $P(A,B) = P(A)P(B) = P(B)P(A) = P(B,A)$, we have $P(A,B) =
P(B,A) = P(B|A)P(A)$, which leads us to a powerful result in
probability; *Bayes Theorem*.

+ *Corollary*: Bayes Theorem. Given two events, $A$ and $B$, we may
  represent the probability of $A$ given $B$ in terms of the
  probability of $B$ given $A$.
  \[ P(A|B) = \frac{ P(A) P(B|A) }{ P(B) } \]

Intuitively, there are likely to be situations where our
degree-of-belief in the state of one variable does not affect our
belief in another; in this case the variables are said to be
/independent/ from one another.

+ *Definition*: Independence. Two variables, $x,y$ are said to be
  indendent iff \[ P(x,y) = P(x) P(y) \]

+ *Corollary*: For two independent variables $x,y$, \[P(x|y) = P(x,y)
  / P(y) = (P(x)P(y))/P(y) = P(x).\]

There may also be situations where two variables become independent if
the state of a third variable is known.

+ *Definition*: Conditional independence. Two variables, $x,y$ are
  said to be conditionally independent given a third variable, $z$, if
  \[ P(x,y | z) = P(x|z)P(y|z).\] We can denote conditional
  independence as $x\!\perp\!\!\!\perp\!y \‚ÄÅ| z$.

* Kernels

#+LATEX_ATTR: :options [Kernel]
#+BEGIN_definition
For all variables $x$ and $x'$ in the input   space, $\set{X}$ of a probability distribution, a mapping $k:  \set{X} \times \set{X} \to \mathbb{R}$ is a kernel function.
#+END_definition

If the kernel function can be written in the form of a dot-product
between two /feature maps/, $\phi: \set{X} \to \set{V}$, \[ k(x, x') =
\langle \phi(x), \phi(x') \rangle v, \] for $\set{V}$ some inner
product space, then we can perform the "kernel trick", allowing us to
define the kernel in terms of the inner products within the data,
without resorting to an external coordinate system.

* Structured probability distributions

A complicated joint probability distribution can always be factorised
into lower-dimensional factor distributions. For example, \[ p(a,b,c)
= p(a | b , c) p(b, c) = p(a | b, c) p (b | c) p(c).\] We can then
represent these factorisations in the form of a directed graph, with
\[ c \to b \to a \] representing $p(a,b,c)$. In such a graph we use
the direction of an arrow to imply a conditional relationship. When
expressed in this form we can call the probability distribution a
belief network.

+ *Definition*: Belief network. A belief network is a probability
  distribution of the form \[ p(x_{1}, \dots, x_{N}) = \prod_{i=1}^{N}
  p(x_{i} | pa(x_{i})), \] where $pa(x)$ represents the parental set
  of the variable $x$; that is, the set of all variables in the graph
  which have a directed edge ending at $x$, or the set of all
  variables on which $x$ is directly conditional.

Clearly not all probability distributions will take the above form,
however, as there may be conditional independences within the
structure of the distribution, leading to factors dropping out. We can
construct a belief network from knowledge of these independence
constraints, starting with a fully connected graph of all variables in
a problem, and then removing edges which connect independent
variables.


* Bayesian inference

Bayesian inference is a method of statistical inference which is
well-suited to situations where a body of evidence grows over time,
with new results updating previous understanding of some phenomenon,
and as such is well suited to the analysis of experimental data, and
is well suited to the analysis of gravitational wave data, where
measurements are frequently made at different sensitivities during
different observing runs.

If we have some hypothesis, some parameters of the hypothesis, $I$
(so-called hyperparameters) and some experimental data, we can
determine the probability of the hypothesis via
\begin{equation}
  \label{eq:bayestheorem}
  p(\text{hypothesis} | \text{data}, I) \propto p( \text{data} | \text{hypothesis}) \times p(\text{hypothesis}, I)
\end{equation}
where $p(\text{data} | \text{hypothesis})$ represents the likelihood
of the data, in-effect the degree to which we trust the measurements,
for example, and $p(\text{hypothesis}|I)$ represents the /prior/
probability, which represents the understanding of the probability of
the hypothesis before the experiment was
conducted. $p({\text{hypothesis} | \text{data}, I)$ is the /posterior/
probability of the hypothesis cite:skilling2006data.

Bayesian inference can then be used as a powerful method for /model
  selection/, where the posterior probabilities of two competing
  models are compared, with a greater posterior probability indicating
  greater support for a given model.
