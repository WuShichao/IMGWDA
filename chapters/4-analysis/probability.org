#+TITLE: Probability

* Probability 
In this work we consider "probability" as the notion of our
degree-of-belief in some event or occurence. For example, a coin, when
tossed, can take one of two states when it lands, which we call
"heads" or "tails"; we denote these two states $H$ and $T$. Assuming
that we have no knowledge of a reason why the coin would fall one way
rather than the other, we are forced to conclude that we have no more
reason to believe that the coin will fall heads, compared to it
falling tails.

With this philosophy, we can construct a mathematical model of
probability. We make some additional demands on this model, one of
which is that an event which is considered "certain" should have a
probability of $1$, maintaining consistency with boolean
logic. Likewise, if we are certain that an event will not occur, then
we assign it a value $0$. 

#+ATTR_LATEX: :options [Sample Space]
#+BEGIN_definition
If a variable can take on a number of
different values, then the set of all its possible values is called
the sample space. In any given problem, the sample space composes
the universe set, and is often denoted $\Omega$.
#+END_definition

#+ATTR_LATEX: :options [State]
#+BEGIN_definition
A state (or event) is any subset of zero or more elements from the sample space. A state containing one element  is a simple state, while one containing more than one element is a compound state. The states form a $\sigma$-algebra on the sample space.
#+END_definition

#+ATTR_LATEX: :options [Probability]
#+BEGIN_definition
Let $x$ be some variable which is capable
  of having a state $E \in \Omega$ for $\Omega$ the /sample space/ of
  the variable. Probability $P$ is a mapping $P: E \to [0,1]$ which
  assigns a real value between $0$ and $1$ to every $E \in \Omega$. We
  place three constraints on the form of this mapping:
  1. For every $E \in \Omega$, $P(E) \geq 0$.
  2. $P(\Omega) = \sum_{E \in \Omega} P(E) = 1$.
  3. If $E_{i} \cap E_{j} = \emptyset$ for $E_{i}, E_{j} \in \Omega$, then $P(\cup E_{i}) = \sum P(E_{i})$
#+END_definition

#+ATTR_LATEX: :options [Joint probability]
#+BEGIN_definition
The probability of two events, $A,B
  \in \Omega$, occuring is a "joint probability", and is computed as \[ P(A \cap B) = P(A, B) = P(A) P(B). \]
#+END_definition

+ *Corollary*: For two events, $A,B \in \Omega$, we have $P(A \cup B) = P(A) + P(B) - P(A \cap B)$

In the case that we have two events which occur with some dependence
between them, we can form a "conditional probability", for example, if
there can be no smoke without fire, then the probability of smoke can
be conditional on the probability of fire.

#+ATTR_LATEX: :options [Conditional probability]
#+LABEL: def:probability:conditional
#+BEGIN_definition
Given two events, $A,B \in
\Omega$, then the probability of $A$ /given/ $B$ is \[ P(A | B) =
\frac{ P(A,B) }{ p(B) }. \] If $P(B) = 0$ then $P(A)$ is undefined.
#+END_definition

Given that $P(A,B) = P(A)P(B) = P(B)P(A) = P(B,A)$, we have $P(A,B) =
P(B,A) = P(B|A)P(A)$, which leads us to a powerful result in
probability; *Bayes Theorem*.

#+ATTR_LATEX: :options [Bayes Theorem]
#+LABEL: the:probability:bayes-theorem
#+BEGIN_theorem
Given two events, $A$ and $B$, we may represent the probability of $A$ given $B$ in terms of the probability of $B$ given $A$.
  \[ P(A|B) = \frac{ P(A) P(B|A) }{ P(B) } \]
#+END_theorem
Intuitively, there are likely to be situations where our
degree-of-belief in the state of one variable does not affect our
belief in another; in this case the variables are said to be
/independent/ from one another.

#+ATTR_LATEX: :options [Independence]
#+BEGIN_definition 
Two variables, $x,y$ are said to be
indendent iff \[ P(x,y) = P(x) P(y) \]
#+END_definition

+ *Corollary*: For two independent variables $x,y$, \[P(x|y) = P(x,y)
  / P(y) = (P(x)P(y))/P(y) = P(x).\]

There may also be situations where two variables become independent if
the state of a third variable is known.

#+ATTR_LATEX: :options [Conditional independence]
#+BEGIN_definition
Two variables, $x,y$ are said to be conditionally independent given a third variable, $z$, if \[ P(x,y | z) = P(x|z)P(y|z).\] 
We can denote conditional independence as $x\!\perp\!\!\!\perp\!y \‚ÄÅ| z$.
#+END_definition

* Prior knowledge
** Uninformative priors
   :PROPERTIES:
   :CUSTOM_ID: sec:probability:priors:uninformative
   :END:

*** Fisher information
*** Haldane and Jeffreys priors

* Information

  Understanding how informative an random variable, $X$ is can provide insight into how well observations of that variable will inform our knowledge of the probability distribution from which it is drawn.
  
  #+ATTR_LATEX: :options [Fisher information]
  #+BEGIN_definition
  Given a abbr:pdf, $p$, for a random variable, $X$, which is parameterised by a variable $\theta$, $f(X, \theta)$, the /score/, $V$ of the abbr:pdf is defined
  \begin{equation}
  \label{eq:probability:score}
  V(\theta, X) = \frac{\partial}{\partial X} \log p(X, \theta)
  \end{equation}
  The variance of the score is the /Fisher information/ of the distribution:
  \begin{equation}
  \label{eq:probability:fisher}
  I(\theta, X) = \mathbb{E}(V^{2} | \theta) = \int V^{2} p(X, \theta) \dd{x}
  \end{equation}
  #+END_definition

  Knowledge of the Fisher information for a given distribution is particuarly valuable in selecting an /uninformative prior/ (see section ref:sec:probability:priors:uninformative) when designing a Bayesian  analysis, where it can be valuable for the prior probability distribution to contribute no information to the inference.

  #+ATTR_LATEX: :options [Shannon information]
  #+BEGIN_definition
  Given a abbr:pdf, $p$, for a random variable $X$ the /Shannon information content/ of a given value $x$ of $X$ is defined as
  \begin{equation}
  \label{eq:probability:shannon}
  h(x) = \log_{2} p^{-1}(x)
  \end{equation}
  where the information is measured in /bits/ (assuming that a base-2 logarithm is used; if the natural logarithm is used the units are /nats/, and the base-10 gives rise to the /dit/).
  #+END_definition

  #+ATTR_LATEX: :options [Entropy]
  #+BEGIN_definition
  The entropy of a random variable $X$ with a abbr:pdf, $p$ is the average Shannon information of the random variable across all its possible values:
  \begin{equation}
  H(X) = \int p(X) h(X) \dd X
  \end{equation}
  taking $0 \log (1/0) \equiv 0$.
  #+END_definition

** KL / SJ Divergence



* Feature spaces and Kernels
#+LABEL: sec:probability:features-and-kernels
#+NAME: sec:probability:features-and-kernels

A feature map is a projection from a lower-dimensional data space to a higher-dimensional one, which can be represented by a mapping, $\phi$. 

#+LATEX_ATTR: :options [Feature map]
#+BEGIN_definition
For a $D$-dimensional vector $\vec{x}$, a feature map, $\phi : \mathbb{R}^{D} \to \mathbb{R}^{N}$ is a mapping which projects $\vec{x}$ into an $N$-dimensional space, the \emph{feature space}.
#+END_definition

This can be a valuable technique in statistical regression and classification, where data may become linearly separable in a higher dimensional space, or can be described by a simpler function than in the original data space. 
An example of such a mapping is $\phi : \mathbb{R} \to \mathbb{R}^{3}, \quad \phi(x) = (1, x, x^2)^{\transpose}$, which can be used to implement quadratic regression, as 
\begin{equation}
\label{eq:quadratic-regression}
f(\vec{x}) = w_0 + w_{1} \vec{x} + w_{2} \vec{x} = \phi(\vec{x})^{\transpose} \cdot \vec{w}
\end{equation}
which remains linear (and therefore analytically solvable) provided $\phi$ is independent of $\vec{w}$.

*Some notes to bridge the jump from feature maps to kernels*.

#+LATEX_ATTR: :options [Kernel]
#+BEGIN_definition
For all variables $x$ and $x'$ in the input   space, $\set{X}$ of a probability distribution, a mapping $k:  \set{X} \times \set{X} \to \mathbb{R}$ is a kernel function.
#+END_definition

If the kernel function can be written in the form of a dot-product between two /feature maps/, $\phi: \set{X} \to \set{V}$, 
\[ k(x, x') = \langle \phi(x), \phi(x') \rangle v, \] 
for $\set{V}$ some inner product space, then we can perform the "kernel trick", allowing us to define the kernel in terms of the inner products within the data, without resorting to an external coordinate system.

* Structured probability distributions
  :PROPERTIES:
  :CUSTOM_ID: sec:probability:structured
  :END:

A complicated joint probability distribution can often be factorised into lower-dimensional factor distributions if there are conditional independences within the model which that distribution describes.
For example, 
\[ 
p(a,b,c) = p(a | b , c) p(b, c) = p(a | b, c) p (b | c) p(c).
\] 
We can then represent these factorisations in the form of a directed graph, with
\[ c \to b \to a \] 
representing $p(a,b,c)$. In such a graph we use the direction of an arrow to imply a conditional relationship. 
When expressed in this form we can call the probability distribution a belief network, or a graphical model.

As a concrete (if rather naive) example, consider a situation in which observations are made continuously over the whole sky with two detectors. 
One is sensitive to gls:gravitational-wave emission, and the other to gamma ray emission.
An observing program is estabilised to analyse transient signals detected with one or both of these telescopes, with the belief that gravitational wave bursts can be produced by either a binary neutron star coalesence, or a binary black hole coalesence.

A simple model is constructed which contains four variables
1) $\Gamma \in \{ 0, 1 \}$ which takes the value $1$ iff a gamma ray burst is detected,
2) $G \in \{ 0, 1 \}$ which takes the value $1$ iff a gravitational wave burst is detected,
3) $B \in \{ 0, 1 \}$ which takes the value $1$ iff a black hole coalescence has occurred, and
4) $N \in \{ 0, 1 \}$ which takes the value $1$ iff a neutron star coalescence has occurred.

The joint probability distribution of this model is then $p(\Gamma, G, B, N)$, however we can break this down into a structured form by applying the definition of conditional probability (definition ref:def:probability:conditional),

\begin{align}
\label{probability:structured:example:breakdown}
p ( \Gamma, G, B, N) &= p(\Gamma | G, B, N) p(G, B, N)\\
                     &= p(\Gamma | G, B, N) p(G | B, N) p(B, N) \\
                     &= p(\Gamma | G, B, N) p(G | B, N) p(B | N) p(N)
\end{align}

We can represent this model as a graph

\begin{center}
\begin{tikzpicture}

	 \node[obs] (gamma) {$\Gamma$};	 	
	 \node[obs, right = of gamma] (G)     {$G$};

	 \node[latent, above = of G] (B) {$B$};
	 \node[latent, above = of gamma] (N) {$N$};

	 \edge{B} {G};
	 \edge{B} {gamma};
	 \edge{G} {gamma};
	 \edge{N} {G};
	 \edge{N} {B};
	 \edge{N} {gamma};

\end{tikzpicture}
\end{center}

Our observers have access to a number of up to date astrophysical theories which they can use to further develop the model; these place /conditional independence/ constraints on the model.
- Binary black hole coalescences and binary neutron star coalescences are independent (one does not cause the other)
This statement implies that $p(B | N) = p(B)$, and $p(N | B) = p(N)$, which we can represent in the graphical form of the model by removing the edge connecting $B$ and $N$.

\begin{center}
\begin{tikzpicture}

	 \node[obs] (gamma) {$\Gamma$};	 	
	 \node[obs, right = of gamma] (G)     {$G$};

	 \node[latent, above = of G] (B) {$B$};
	 \node[latent, above = of gamma] (N) {$N$};

	 \edge{B} {G};
	 \edge{B} {gamma};
	 \edge{G} {gamma};
	 \edge{N} {G};
	 \edge{N} {gamma};

\end{tikzpicture}
\end{center}

- A binary black hole coalescence does not produce any electromagnetic emission (and therefore cannot produce a gamma ray burst)
This statement implies that $p(\Gamma | B) = p(\Gamma)$, which can be represented in the graphical form of the model by removing the edge connecting $\Gamma$ and $B$.

\begin{center}
\begin{tikzpicture}

	 \node[obs] (gamma) {$\Gamma$};	 	
	 \node[obs, right = of gamma] (G)     {$G$};

	 \node[latent, above = of G] (B) {$B$};
	 \node[latent, above = of gamma] (N) {$N$};

	 \edge{B} {G};
	 \edge{G} {gamma};
	 \edge{N} {G};
	 \edge{N} {gamma};

\end{tikzpicture}
\end{center}


These two constraints considerably simplify the model, and we are now left with the distribution in the form 
\begin{equation}
\label{probability:structured:example:final}
p ( \Gamma, G, B, N) = p(\Gamma | N, G) p(G | N, B) p(B) p(N),
\end{equation}
which is easily interpreted from the graphical form of the model, but could have been tedious to derive algebraically. 

We can define a belief network more generally as follows.
#+LATEX_ATTR: :options [Belief Network]
#+BEGIN_definition
#+LABEL: probability:structured:belief-network
A belief network is a probability distribution of the form 
\[ p(x_{1}, \dots, x_{N}) = \prod_{i=1}^{N} p(x_{i} | pa(x_{i})), \] 
where $pa(x)$ represents the parental set of the variable $x$; that is, the set of all variables in the graph which have a directed edge ending at $x$, or the set of all variables on which $x$ is directly conditional.
#+END_definition

** Equivalence of graphical models
   :PROPERTIES:
   :CUSTOM_ID: sec:probability:structured:equivalence
   :END:

   An important caveat with the use of graphical models is that two graphically distinct models may be mathematically equivalent. 
   The reason for this becomes clear when considering the procedure used to factorise the probability distribution starting at equation ref:probability:structured:example:breakdown.
   If we had chosen to re-arrange the variables such that the joint distribution was $p(N,B,G, \Gamma)$ we would have been left with a factorised distribution in which the arrows of the graph pointed in opposite directions, yet this is clearly still the same probability distribution, since probabilities are commutative.
   To overcome this problem we need to have a definition of equivalence in the graph. 
   A suitable definition is that of /Markov equivalence/ cite:barberBRML2012:
   #+ATTR_LATEX: :options [Markov equivalence]
   #+LABEL: def:probability:structured:markov-equivalence
   #+BEGIN_definition
   Two graphs are Markov equivalent if they both represent the same set of conditional independence statements.
   #+END_definition

   Clearly some method to determine this graphically is warranted. To do so it is helpful to define a (rather judgementally-named) property:

   #+ATTR_LATEX: :options [Immorality]
   #+BEGIN_definition
   Consider three nodes, $A$, $B$, and $C$ in a abbr:dag. If $C$ is a child of both $A$ and $B$, but $A$ and $B$ are not directly connected, then the configuration $A \rightarrow C \leftarrow B$ is denoted an immorality.
   #+END_definition

   In order to determine Markov equivalence we remove all of the directionality from the edges of the graph, producing the skeleton graph. 
   Two graphs are Markov equivalent if they share the same skeleton, and if they share the same set of immoralities.

   # We can construct a belief network from knowledge of these independence constraints, starting with a fully connected graph of all variables in a problem, and then removing edges which connect independent variables.

* Inference
  :PROPERTIES:
  :CUSTOM_ID: sec:probability:inference
  :END:

  In section ref:sec:probability:structured we introduced a probabilistic model which consisted of the joint probability of all of the model parameters.
  Such a model if some of the quantities in the model are unknown, and therefore the probability of some of the modelled outcomes can be calculated from the model.

  Taking the example of joint abbr:gw and gamma ray observations, if we know the probability that at any given time there will be a abbr:bns event, we can infer the probability that a gamma ray burst and a abbr:gw burst will occur.
  A model of this form is often considered a "forward model", in that it predicts the probability of an observable, and calculation through the graph follows the arrows.
  While such forward models are of considerable utility when attempting to make predictions about unknown variables, often with pre-existing data, they are unable to answer a question such as "given that I have seen a gravitational wave, but no gamma ray burst, what is the probability that I have observed a abbr:bbh event?".
  In order to answer such a question we must traverse the graphical model /backwards/, against the direction of the arrows. This process is known as /inference/.

  In order to produce the /reverse model/ we can turn to Bayes theorem (theorem ref:the:probability:bayes-theorem). This allows us to derive an expression for $p(B = 1 | G = 1, \Gamma = 0)$, that is, the probability that we observe a abbr:bbh given that we've observed a abbr:gw but no gamma ray burst.
  \begin{align}
  \label{eq:probability:inference:bayes-example}
  p(B = 1 | G = 1, \Gamma = 0) &= \frac{p(B=1,G=1,\Gamma=0)}{p(G=1, \Gamma=0)} \\
			       &= \frac{\int_{N} p(B=1,G=1,\Gamma=0, N)}{ \int_{B,N} p(G=1, \Gamma=0, B, N)} \\
			       &= \frac{\int_{N} p(\Gamma=0 | G=1, B=1, N) p(G =1 | B=1, N) p(B=1 | N) p(N)} 
				       {\int_{B,N} p(\Gamma=0 | G=1, B, N) p(G =1 | B, N) p(B | N) p(N)}      \\
			       &= \frac{\int_{N} p(\Gamma=0 | G=1, B=1, N) p(G =1 | B=1, N) p(B=1 | N) p(N)}
				       {\int_{B,N} p(\Gamma=0 | G=1, B, N) p(G =1 | B, N) p(N)}
  \end{align}
  the probability $p(B = 1 | G = 1, \Gamma = 0)$ is called the /posterior probability of $B$/.

  Inference which is based on Bayes theorem, is a method of statistical inference which is well-suited to situations where a body of evidence grows over time, with new results updating previous understanding of some phenomenon, and as such is well suited to the analysis of experimental data.
  It is well suited to the analysis of gravitational wave data, where measurements are frequently made at different sensitivities during different observing runs.

  If we have some hypothesis, some parameters of the hypothesis, $I$ (so-called hyperparameters) and some experimental data, we can
  determine the probability of the hypothesis via 

  \begin{equation}
    \label{eq:probability:inference:bayes-theorem-hypothesis}
    p(\text{hypothesis} | \text{data}, I) \propto p( \text{data} | \text{hypothesis}) \times p(\text{hypothesis}, I)
  \end{equation}

  where $p(\text{data} | \text{hypothesis})$ represents the likelihood of the data, in-effect the degree to which we trust the measurements, for example, and $p(\text{hypothesis}|I)$ represents the /prior/ probability, which represents the understanding of the probability of the hypothesis before the experiment was conducted. $p({\text{hypothesis} | \text{data}, I)$ is the /posterior/
  probability of the hypothesis cite:skilling2006data.

  Bayesian inference can then be used as a powerful method for /model selection/, where the posterior probabilities of two competing
  models are compared, with a greater posterior probability indicating greater support for a given model.

* Approximate inference methods
** MCMC
** Nested sampling

* Hierarchical modelling


* Bayesian Linear regression
  :PROPERTIES:
  :CUSTOM_ID: sec:probability:blr
  :END:

To motivate the development of Gaussian processes we first present the
problem of linear regression, and how this task may be performed in a
Bayesian framework.

A very simple linear model has the form

#+NAME:eq-simple-linear-model
\begin{equation}
  y_i = m x_i + c
\end{equation}

for each observation, $y_i$, which are made at a location in parameter
space $x_i$, where $m, c \in \mathbb{R}$ are the parameters of our
model. This model attempts to describe the observations by fitting a
polynomial of order one to the data, however, we
may reasonably want to generalise our model to allow higher orders of
polynomial, and doing so achieve a model of the form

\begin{equation}
\label{eq:less-simple-linear-model}
  y_i = \sum_{d=1}^p w_d x_{d,i} = \vec{x}_i \vdot \vec{w}
\end{equation}

where the various parameters are now folded into a vector, $\vec{w}$, which is called the 
weight vector. A further generalisation may be made, to allow for more
complicated forms of model, by substituting the vector $\vec{x}$ for a
matrix $\mat{X}$, the /design matrix/, which can take an arbitrary form,
so our model becomes

$$\label{eq:general-noiseless-linear}
  \vec{y} = \mat{X} \vec{w}$$

Finally, we can introduce a term to account for any uncertainty in the
measurement of the observations, $\vec{\epsilon}$, giving a complete
linear model

$$\label{eq:linear-model}
  \vec{y} = \mat{X} \vec{w} + \vec{\epsilon}$$

The quantities $\vec{y}$, $\mat{X}$, and $\vec{\epsilon}$ are well
understood, but to have a complete and useful model we must find the
values of the model parameters which best explain the data; this problem
is regression. In a Bayesian framework we must assign a prior to each
parameter, which represents our pre-existing knowledge of the situation.
A sensible choice might be a normal, or /Gaussian/, distribution, with a
mean of zero, and a variance $\sigma_i^2$:
$$w_i \sim \mathcal{G}(0, \sigma_i^2).$$
